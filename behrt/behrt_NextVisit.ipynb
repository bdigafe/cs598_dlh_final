{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDIyndGVBJZh"
      },
      "source": [
        "### Task 3: Next 6-months prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtTLL2DrBJZk"
      },
      "source": [
        "#### Fine tunning\n",
        "\n",
        "BERT generates word embeddings for the vocabulary words included in the corpus. In other words, each word or medical diganoses in our case, is mapped to a vector. In contrast to Word2Vec, BERT generates code that captures the local context of words and therefore provides a better representation of the word.\n",
        "\n",
        "MLM (Masked Language Model) is used for optimization, which we already performed on the 2nd task. In this step, we will add a task to predict the diagnosis codes after 6-months from a randomly selected visit date. This new task will be trained and the word embeddings will be fine tuned as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnOQmjBmQPrk"
      },
      "outputs": [],
      "source": [
        "local_mode = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCUA0c8cBgHD",
        "outputId": "5cf5c646-e1be-4f1c-e9fd-364156ade707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘commons’: File exists\n",
            "--2023-05-03 03:37:24--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/commons/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10654 (10K) [text/plain]\n",
            "Saving to: ‘commons/utils.py.1’\n",
            "\n",
            "utils.py.1          100%[===================>]  10.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-03 03:37:24 (117 MB/s) - ‘commons/utils.py.1’ saved [10654/10654]\n",
            "\n",
            "--2023-05-03 03:37:24--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/commons/__init__.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 0 [text/plain]\n",
            "Saving to: ‘commons/__init__.py.1’\n",
            "\n",
            "__init__.py.1           [ <=>                ]       0  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-03 03:37:24 (0.00 B/s) - ‘commons/__init__.py.1’ saved [0/0]\n",
            "\n",
            "mkdir: cannot create directory ‘models’: File exists\n",
            "--2023-05-03 03:37:24--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/MLM.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6199 (6.1K) [text/plain]\n",
            "Saving to: ‘models/MLM.py.1’\n",
            "\n",
            "MLM.py.1            100%[===================>]   6.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-03 03:37:25 (68.1 MB/s) - ‘models/MLM.py.1’ saved [6199/6199]\n",
            "\n",
            "--2023-05-03 03:37:25--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/NextXVisit.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6247 (6.1K) [text/plain]\n",
            "Saving to: ‘models/NextXVisit.py.1’\n",
            "\n",
            "NextXVisit.py.1     100%[===================>]   6.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-03 03:37:25 (70.6 MB/s) - ‘models/NextXVisit.py.1’ saved [6247/6247]\n",
            "\n",
            "--2023-05-03 03:37:25--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/optimizer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 772 [text/plain]\n",
            "Saving to: ‘models/optimizer.py.1’\n",
            "\n",
            "optimizer.py.1      100%[===================>]     772  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-03 03:37:25 (49.0 MB/s) - ‘models/optimizer.py.1’ saved [772/772]\n",
            "\n",
            "--2023-05-03 03:37:25--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/BertConfig.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 953 [text/plain]\n",
            "Saving to: ‘models/BertConfig.py.1’\n",
            "\n",
            "BertConfig.py.1     100%[===================>]     953  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-03 03:37:25 (64.9 MB/s) - ‘models/BertConfig.py.1’ saved [953/953]\n",
            "\n",
            "--2023-05-03 03:37:25--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/__init__.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 0 [text/plain]\n",
            "Saving to: ‘models/__init__.py.1’\n",
            "\n",
            "__init__.py.1           [ <=>                ]       0  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-03 03:37:25 (0.00 B/s) - ‘models/__init__.py.1’ saved [0/0]\n",
            "\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "--2023-05-03 03:37:26--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/ages.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12929552 (12M) [application/octet-stream]\n",
            "Saving to: ‘data/ages.pkl.1’\n",
            "\n",
            "ages.pkl.1          100%[===================>]  12.33M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-05-03 03:37:26 (224 MB/s) - ‘data/ages.pkl.1’ saved [12929552/12929552]\n",
            "\n",
            "--2023-05-03 03:37:26--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/concept.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15122701 (14M) [application/octet-stream]\n",
            "Saving to: ‘data/concept.pkl.1’\n",
            "\n",
            "concept.pkl.1       100%[===================>]  14.42M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-05-03 03:37:26 (229 MB/s) - ‘data/concept.pkl.1’ saved [15122701/15122701]\n",
            "\n",
            "--2023-05-03 03:37:26--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/condition_codes.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18889 (18K) [application/octet-stream]\n",
            "Saving to: ‘data/condition_codes.pkl.1’\n",
            "\n",
            "condition_codes.pkl 100%[===================>]  18.45K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-03 03:37:26 (109 MB/s) - ‘data/condition_codes.pkl.1’ saved [18889/18889]\n",
            "\n",
            "--2023-05-03 03:37:26--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/conditions.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10038026 (9.6M) [application/octet-stream]\n",
            "Saving to: ‘data/conditions.pkl.1’\n",
            "\n",
            "conditions.pkl.1    100%[===================>]   9.57M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-05-03 03:37:27 (188 MB/s) - ‘data/conditions.pkl.1’ saved [10038026/10038026]\n",
            "\n",
            "mkdir: cannot create directory ‘saved_models’: File exists\n",
            "--2023-05-03 03:37:27--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/saved_models/mlm128.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17990392 (17M) [application/octet-stream]\n",
            "Saving to: ‘saved_models/mlm128.pt.1’\n",
            "\n",
            "mlm128.pt.1         100%[===================>]  17.16M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-05-03 03:37:27 (268 MB/s) - ‘saved_models/mlm128.pt.1’ saved [17990392/17990392]\n",
            "\n",
            "mkdir: cannot create directory ‘images’: File exists\n",
            "--2023-05-03 03:37:27--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/cdm54.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 912664 (891K) [image/png]\n",
            "Saving to: ‘images/cdm54.png.1’\n",
            "\n",
            "cdm54.png.1         100%[===================>] 891.27K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-05-03 03:37:27 (53.9 MB/s) - ‘images/cdm54.png.1’ saved [912664/912664]\n",
            "\n",
            "--2023-05-03 03:37:27--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/behrt_embeddings.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 450527 (440K) [image/png]\n",
            "Saving to: ‘images/behrt_embeddings.png.1’\n",
            "\n",
            "behrt_embeddings.pn 100%[===================>] 439.97K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-05-03 03:37:28 (34.3 MB/s) - ‘images/behrt_embeddings.png.1’ saved [450527/450527]\n",
            "\n",
            "--2023-05-03 03:37:28--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/behrt_model.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 116319 (114K) [image/png]\n",
            "Saving to: ‘images/behrt_model.png.1’\n",
            "\n",
            "behrt_model.png.1   100%[===================>] 113.59K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2023-05-03 03:37:28 (14.7 MB/s) - ‘images/behrt_model.png.1’ saved [116319/116319]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if not local_mode:\n",
        "  !mkdir commons\n",
        "  !wget -P commons https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/commons/utils.py\n",
        "  !wget -P commons https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/commons/__init__.py\n",
        "\n",
        "  !mkdir models\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/MLM.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/NextXVisit.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/optimizer.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/BertConfig.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/__init__.py\n",
        "\n",
        "  !mkdir data\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/ages.pkl\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/concept.pkl\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/condition_codes.pkl\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/conditions.pkl\n",
        "\n",
        "  !mkdir saved_models\n",
        "  !wget -P saved_models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/saved_models/mlm128.pt\n",
        "\n",
        "  !mkdir images\n",
        "  !wget -P images https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/cdm54.png\n",
        "  !wget -P images https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/behrt_embeddings.png\n",
        "  !wget -P images https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/behrt_model.png\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not local_mode:\n",
        "  %pip install pytorch_pretrained_bert "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWpwpgYeZGJg",
        "outputId": "3e8071de-e392-4b68-f9dc-2c33b10102d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.10/dist-packages (0.6.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2022.10.31)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.27.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.0.0+cu118)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.26.125)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (3.25.2)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.125 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (1.29.125)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.125->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.125->boto3->pytorch_pretrained_bert) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXiIjJdbQPrl"
      },
      "source": [
        "### File and model paramters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IayjuaWgBJZm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import commons.utils as utils\n",
        "from models import optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_pretrained_bert as Bert\n",
        "\n",
        "# set random seed for reproducibility\n",
        "seed = 123\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ['PYTHONASHSEED'] = str(seed)\n",
        "\n",
        "global_params = {\n",
        "    'max_seq_len': 256,\n",
        "    'max_age' : 110,\n",
        "    'age_month' : 1,\n",
        "    'batch_size': 128,\n",
        "    'num_epochs': 20,\n",
        "    'min_visit': 5,\n",
        "    'gradient_accumulation_steps': 1,\n",
        "    'training_sample' : 0,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "}\n",
        "\n",
        "optim_config = {\n",
        "    'lr': 3e-5,\n",
        "    'warmup_proportion': 0.1,\n",
        "    'weight_decay': 0.01\n",
        "}\n",
        "\n",
        "file_config = {\n",
        "    'vocab': ('C:/Birhanu/Education/UIL/cs598/Final/data/' if local_mode else 'data/') + 'condition_codes.pkl',\n",
        "    'data': ('C:/Birhanu/Education/UIL/cs598/Final/data/' if local_mode else 'data/' ) + 'conditions.pkl',\n",
        "    'ages' : ('C:/Birhanu/Education/UIL/cs598/Final/data/' if local_mode else 'data/' ) + 'ages.pkl',\n",
        "\n",
        "    'model_path': 'C:/Birhanu/Education/UIL/cs598/Final/saved_models/' if local_mode else 'saved_models/', \n",
        "    'model_name': 'next6m-model',  # model name\n",
        "    \n",
        "    'log_file_name': 'next6m-log',  # log path\n",
        "}\n",
        "\n",
        "# MLM pretrained model path\n",
        "if local_mode:\n",
        "  pretrainModel = 'C:/Birhanu/Education/UIL/cs598/Final/saved_models/mlm128.pt'\n",
        "else:\n",
        "   pretrainModel = 'saved_models/mlm128.pt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkM2z7YqH0K6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6465a582-24ae-4105-c297-010adc96ebd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "print(global_params[\"device\"])\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt_kVGeZBJZo"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf_4wsLcBJZo",
        "outputId": "00f75a37-6e81-471e-c02f-f1df2423f263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conditions vocab file: data/condition_codes.pkl\n",
            "      pid                                              visit  \\\n",
            "0  176102  [35207924, 35211387, SEP, 1569634, SEP, 157606...   \n",
            "1  176104  [35208969, SEP, 35208968, 35208969, SEP, 35208...   \n",
            "2  176106  [35207966, SEP, 45591555, SEP, 1570694, 352089...   \n",
            "3  176107  [1569446, SEP, 1571058, 1574193, 35209007, SEP...   \n",
            "4  176108  [35208190, SEP, 35208481, SEP, 1567866, SEP, 1...   \n",
            "\n",
            "                                                 age                     label  \n",
            "0  [230, 230, 230, 262, 262, 263, 263, 267, 267, ...            [1576063, SEP]  \n",
            "1  [623, 623, 623, 623, 623, 623, 623, 623, 625, ...   [1568344, 1572256, SEP]  \n",
            "2  [455, 455, 460, 460, 467, 467, 467, 467, 467, ...  [1570694, 35208968, SEP]  \n",
            "3  [407, 407, 408, 408, 408, 408, 409, 409, 413, ...            [1570790, SEP]  \n",
            "4  [779, 779, 784, 784, 795, 795, 796, 796, 797, ...   [1567750, 1569558, SEP]  \n"
          ]
        }
      ],
      "source": [
        "# Load conditions codes (word vocabs)\n",
        "print(f\"Conditions vocab file: {file_config['vocab']}\")\n",
        "vocab_conditions = utils.get_codes_vocab(file_config[\"vocab\"])\n",
        "code2idx = vocab_conditions[\"token2idx\"]\n",
        "#code2idx = utils.remove_system_codes_from_token_dict(code2idx)\n",
        "\n",
        "# Create age vocab\n",
        "vocab_age = utils.age_vocab(global_params[\"max_age\"], global_params[\"age_month\"])\n",
        "age2idx = vocab_age[0]\n",
        "\n",
        "# Load data\n",
        "data_conditions = utils.load_data(file_config[\"data\"], sample_size=global_params[\"training_sample\"])\n",
        "data_age_seqs = utils.load_data(file_config[\"ages\"], sample_size=global_params[\"training_sample\"])\n",
        "\n",
        "# Split data into train, validation, and test\n",
        "train_data, test_data = utils.split_data(data_conditions, data_age_seqs, train_ratio=0.8, min_size=3)\n",
        "print(train_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-yUy_LjQPrn"
      },
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    # number of disease + symbols for word embedding\n",
        "    'vocab_size': len(vocab_conditions['token2idx'].keys()),\n",
        "\n",
        "    # word embedding and seg embedding hidden size\n",
        "    'hidden_size': 288, \n",
        "\n",
        "    # number of vocab for seg embedding\n",
        "    'seg_vocab_size': 2, \n",
        "\n",
        "    # number of vocab for age embedding\n",
        "    'age_vocab_size': len(vocab_age[0].keys()),\n",
        "\n",
        "    # maximum number of tokens\n",
        "    'max_position_embedding': global_params['max_seq_len'],\n",
        "\n",
        "    # dropout rate\n",
        "    'hidden_dropout_prob': 0.2, \n",
        "\n",
        "    # number of multi-head attention layers required\n",
        "    'num_hidden_layers': 6,  \n",
        "\n",
        "    # number of attention heads\n",
        "    'num_attention_heads': 12,\n",
        "\n",
        "    # multi-head attention dropout rate  \n",
        "    'attention_probs_dropout_prob': 0.22,  \n",
        "\n",
        "    # the size of the \"intermediate\" layer in the transformer encoder\n",
        "    'intermediate_size': 512,\n",
        "\n",
        "    # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
        "    'hidden_act': 'gelu',\n",
        "    'initializer_range': 0.02,  # parameter weight initializer range\n",
        "}\n",
        "\n",
        "feature_dict = {\n",
        "    'age': True,\n",
        "    'seg': True,\n",
        "    'posi': True\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smgL5LjrQPrn"
      },
      "outputs": [],
      "source": [
        "class NextVisit(Dataset):\n",
        "    def __init__(self, token2idx, diag2idx, age2idx, dataframe, max_len, max_age=110, min_visit=5):\n",
        "        # dataframe preproecssing\n",
        "        # filter out the patient with number of visits less than min_visit\n",
        "        self.vocab = token2idx\n",
        "        self.label_vocab = diag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.code = dataframe.visit\n",
        "        self.age = dataframe.age\n",
        "        self.label = dataframe.label\n",
        "        self.patid = dataframe.pid\n",
        "\n",
        "        self.age2idx = age2idx\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        return: age, code, position, segmentation, mask, label\n",
        "        \"\"\"\n",
        "        # cut data\n",
        "        age = self.age[index]\n",
        "        code = self.code[index]\n",
        "        label = self.label[index]\n",
        "        patid = self.patid[index]\n",
        "\n",
        "        # extract data\n",
        "        age = age[(-self.max_len+1):]\n",
        "        code = code[(-self.max_len+1):]\n",
        "\n",
        "        # avoid data cut with first element to be 'SEP'\n",
        "        if code[0] != 'SEP':\n",
        "            code = np.append(np.array(['CLS']), code)\n",
        "            age = np.append(np.array(age[0]), age)\n",
        "        else:\n",
        "            code[0] = 'CLS'\n",
        "\n",
        "        # mask 0:len(code) to 1, padding to be 0\n",
        "        mask = np.ones(self.max_len)\n",
        "        mask[len(code):] = 0\n",
        "\n",
        "        # pad age sequence and code sequence\n",
        "        age = utils.seq_padding(age, self.max_len, token2idx=self.age2idx)\n",
        "\n",
        "        tokens, code = utils.code2index(code, self.vocab)\n",
        "        _, label = utils.code2index(label, self.label_vocab)\n",
        "\n",
        "        # get position code and segment code\n",
        "        tokens = utils.seq_padding(tokens, self.max_len)\n",
        "        position = utils.position_idx(tokens)\n",
        "        segment = utils.index_seg(tokens)\n",
        "\n",
        "        # pad code and label\n",
        "        code = utils.seq_padding(code, self.max_len, symbol=self.vocab['PAD'])\n",
        "        label = utils.seq_padding(label, self.max_len, symbol=-1)\n",
        "\n",
        "        return torch.LongTensor(age), \\\n",
        "              torch.LongTensor(code), \\\n",
        "              torch.LongTensor(position), \\\n",
        "              torch.LongTensor(segment), \\\n",
        "              torch.LongTensor(mask), \\\n",
        "              torch.LongTensor(label), \\\n",
        "              torch.LongTensor([int(patid)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.code)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test batch data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def inspect_batch():\n",
        "    dataset = NextVisit(token2idx=code2idx, diag2idx=code2idx,\n",
        "                 age2idx=age2idx, dataframe=train_data, \n",
        "                 max_len=global_params['max_seq_len'])\n",
        "    \n",
        "    train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)\n",
        "    loader_iter = iter(train_loader)\n",
        "    batch = next(loader_iter)\n",
        "\n",
        "    batch = tuple(t.to(global_params['device']) for t in batch)\n",
        "    age_ids, input_ids, posi_ids, segment_ids, attMask, label, masked_label = batch\n",
        "    \n",
        "    # Token codes\n",
        "    p = 15\n",
        "    print(f\"Input:\\n {input_ids}\\n\")\n",
        "    print(f\"Age:\\n{age_ids}\\n\")\n",
        "    print(f\"Positions:\\n{posi_ids}\\n\")\n",
        "    print(f\"Segments:\\n{segment_ids}\\n\")\n",
        "\n",
        "    print(f\"attMask: {attMask}\")\n",
        "    print(f\"Labels: {label}\")\n",
        "\n",
        "inspect_batch()\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKVoJMjZVSZ9",
        "outputId": "d50772d8-d6f6-43fd-ed83-f770c126375b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[  0, 238, 275,   1,  96,   1, 203,   1,  78, 163,   1,  80,   1,  76,\n",
            "           1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2]], device='cuda:0')\n",
            "\n",
            "Age:\n",
            "tensor([[232, 232, 232, 232, 264, 264, 265, 265, 269, 269, 269, 270, 270, 270,\n",
            "         270,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0]], device='cuda:0')\n",
            "\n",
            "Positions:\n",
            "tensor([[0, 0, 0, 0, 1, 1, 2, 2, 3, 3, 3, 4, 4, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]], device='cuda:0')\n",
            "\n",
            "Segments:\n",
            "tensor([[0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
            "\n",
            "attMask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
            "Labels: tensor([[203,   1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-b9og3rQPro"
      },
      "outputs": [],
      "source": [
        "class BertConfig(Bert.modeling.BertConfig):\n",
        "    def __init__(self, config):\n",
        "        super(BertConfig, self).__init__(\n",
        "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
        "            hidden_size=config['hidden_size'],\n",
        "            num_hidden_layers=config.get('num_hidden_layers'),\n",
        "            num_attention_heads=config.get('num_attention_heads'),\n",
        "            intermediate_size=config.get('intermediate_size'),\n",
        "            hidden_act=config.get('hidden_act'),\n",
        "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
        "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
        "            max_position_embeddings=config.get('max_position_embedding'),\n",
        "            initializer_range=config.get('initializer_range'),\n",
        "        )\n",
        "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
        "        self.age_vocab_size = config.get('age_vocab_size')\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, segment, age\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, feature_dict):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.feature_dict = feature_dict\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(\n",
        "            config.vocab_size, config.hidden_size)\n",
        "        self.segment_embeddings = nn.Embedding(\n",
        "            config.seg_vocab_size, config.hidden_size)\n",
        "        self.age_embeddings = nn.Embedding(\n",
        "            config.age_vocab_size, config.hidden_size)\n",
        "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
        "            from_pretrained(embeddings=self._init_posi_embedding(\n",
        "                config.max_position_embeddings, config.hidden_size))\n",
        "\n",
        "        self.LayerNorm = Bert.modeling.BertLayerNorm(\n",
        "            config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, word_ids, age_ids=None, seg_ids=None, posi_ids=None, age=True):\n",
        "        if seg_ids is None:\n",
        "            seg_ids = torch.zeros_like(word_ids)\n",
        "        if age_ids is None:\n",
        "            age_ids = torch.zeros_like(word_ids)\n",
        "        if posi_ids is None:\n",
        "            posi_ids = torch.zeros_like(word_ids)\n",
        "\n",
        "        word_embed = self.word_embeddings(word_ids)\n",
        "        segment_embed = self.segment_embeddings(seg_ids)\n",
        "        age_embed = self.age_embeddings(age_ids)\n",
        "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
        "\n",
        "        embeddings = word_embed\n",
        "\n",
        "        if self.feature_dict['age']:\n",
        "            embeddings = embeddings + age_embed\n",
        "        if self.feature_dict['seg']:\n",
        "            embeddings = embeddings + segment_embed\n",
        "        if self.feature_dict['posi']:\n",
        "            embeddings = embeddings + posi_embeddings\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
        "        def even_code(pos, idx):\n",
        "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
        "\n",
        "        def odd_code(pos, idx):\n",
        "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
        "\n",
        "        # initialize position embedding table\n",
        "        lookup_table = np.zeros(\n",
        "            (max_position_embedding, hidden_size), dtype=np.float32)\n",
        "\n",
        "        # reset table parameters with hard encoding\n",
        "        # set even dimension\n",
        "        for pos in range(max_position_embedding):\n",
        "            for idx in np.arange(0, hidden_size, step=2):\n",
        "                lookup_table[pos, idx] = even_code(pos, idx)\n",
        "        # set odd dimension\n",
        "        for pos in range(max_position_embedding):\n",
        "            for idx in np.arange(1, hidden_size, step=2):\n",
        "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
        "\n",
        "        return torch.tensor(lookup_table)\n",
        "\n",
        "\n",
        "class BertModel(Bert.modeling.BertPreTrainedModel):\n",
        "    def __init__(self, config, feature_dict):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config, feature_dict)\n",
        "        self.encoder = Bert.modeling.BertEncoder(config=config)\n",
        "        self.pooler = Bert.modeling.BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if age_ids is None:\n",
        "            age_ids = torch.zeros_like(input_ids)\n",
        "        if seg_ids is None:\n",
        "            seg_ids = torch.zeros_like(input_ids)\n",
        "        if posi_ids is None:\n",
        "            posi_ids = torch.zeros_like(input_ids)\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids, age_ids, seg_ids, posi_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output\n",
        "\n",
        "\n",
        "class BertForMultiLabelPrediction(Bert.modeling.BertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels, feature_dict):\n",
        "\n",
        "        super(BertForMultiLabelPrediction, self).__init__(config)\n",
        "        \n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config, feature_dict)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, age_ids, seg_ids, posi_ids, attention_mask,\n",
        "                                     output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.MultiLabelSoftMarginLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels),\n",
        "                            labels.view(-1, self.num_labels))\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJbUpHkUQPro"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkOCTrpNQPro"
      },
      "outputs": [],
      "source": [
        "Dset = NextVisit(token2idx=code2idx, diag2idx=code2idx,\n",
        "                 age2idx=age2idx, dataframe=train_data, \n",
        "                 max_len=global_params['max_seq_len'])\n",
        "\n",
        "trainload = DataLoader(\n",
        "    dataset=Dset, batch_size=global_params['batch_size'], shuffle=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_9fGilgQPro"
      },
      "outputs": [],
      "source": [
        "Dset = NextVisit(token2idx=code2idx, diag2idx=code2idx,\n",
        "                 age2idx=age2idx, dataframe=test_data, max_len=global_params['max_seq_len'])\n",
        "\n",
        "testload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=False, num_workers=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDLMK-y_QPrp"
      },
      "source": [
        "### Setup Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8FqEkVMQPrp",
        "outputId": "885d722e-9c71-474a-e287-7dcc52d11dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of conditions vocab: 301\n",
            "Size of Age vocab: 1322\n"
          ]
        }
      ],
      "source": [
        "print(f\"Size of conditions vocab: {len(vocab_conditions['token2idx'])}\")\n",
        "print(f\"Size of Age vocab: {len(vocab_age[0].keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3UBaFLSBJZz"
      },
      "outputs": [],
      "source": [
        "conf = BertConfig(model_config)\n",
        "model = BertForMultiLabelPrediction(conf,  len(vocab_conditions[\"token2idx\"]), feature_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbXmFCpWqOYW",
        "outputId": "cff7db0c-3f90-4bab-962e-e08a7b86e8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMultiLabelPrediction(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(301, 288)\n",
              "      (segment_embeddings): Embedding(2, 288)\n",
              "      (age_embeddings): Embedding(1322, 288)\n",
              "      (posi_embeddings): Embedding(256, 288)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (key): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (value): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (dropout): Dropout(p=0.22, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=288, out_features=512, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=512, out_features=288, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=288, out_features=288, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (classifier): Linear(in_features=288, out_features=301, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdFJ-lfOQPrp",
        "outputId": "967b0c1e-a6fb-4d85-9178-7e451d16c801"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# load pretrained model and update weights\n",
        "pretrained_dict = torch.load(pretrainModel)\n",
        "model_dict = model.state_dict()\n",
        "\n",
        "# 1. filter out unnecessary keys\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "\n",
        "# 2. overwrite entries in the existing state dict\n",
        "model_dict.update(pretrained_dict)\n",
        "\n",
        "# 3. load the new state dict\n",
        "model.load_state_dict(model_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRdWVqJuQPrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b3a131-8d34-48ea-ef01-e47a133a665e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n"
          ]
        }
      ],
      "source": [
        "model = model.to(global_params['device'])\n",
        "optim = optimizer.adam(params=list(\n",
        "    model.named_parameters()), config=optim_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4KnzqSUQPrq"
      },
      "source": [
        "### Evaluation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnaTVcG5QPrq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics._plot.roc_curve import RocCurveDisplay\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def precision(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output = sig(logits)\n",
        "    label, output = label.cpu(), output.detach().cpu()\n",
        "    tempprc=  average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return tempprc, output, label\n",
        "\n",
        "def precision_test(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output=sig(logits)\n",
        "    tempprc= average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    roc = roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return tempprc, roc, output, label,\n",
        "\n",
        "def auroc_test(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output=sig(logits)\n",
        "    tempprc= average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    roc = roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return roc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJdYiqy6QPrq"
      },
      "source": [
        "### Multi-hot Label Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjqNifovQPrq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "db30032b-97e8-45d5-fb74-a36088428f1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiLabelBinarizer(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n",
              "                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
              "                             28, 29, ...])"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n",
              "                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
              "                             28, 29, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n",
              "                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
              "                             28, 29, ...])</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "indexes = list(vocab_conditions[\"token2idx\"].values())\n",
        "\n",
        "mlb = MultiLabelBinarizer(classes=list(indexes))\n",
        "mlb.fit([[each] for each in list(indexes)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZG7RjlTQPrq"
      },
      "source": [
        "### Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjKqYlcAQPrq"
      },
      "outputs": [],
      "source": [
        "def train(e):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    temp_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    cnt = 0\n",
        "    for step, batch in enumerate(trainload):\n",
        "        cnt += 1\n",
        "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
        "        targets = torch.tensor(mlb.transform(\n",
        "            targets.numpy()), dtype=torch.float32)\n",
        "\n",
        "        age_ids = age_ids.to(global_params['device'])\n",
        "        input_ids = input_ids.to(global_params['device'])\n",
        "        posi_ids = posi_ids.to(global_params['device'])\n",
        "        segment_ids = segment_ids.to(global_params['device'])\n",
        "        attMask = attMask.to(global_params['device'])\n",
        "        targets = targets.to(global_params['device'])\n",
        "\n",
        "        loss, logits = model(input_ids, age_ids, segment_ids,\n",
        "                             posi_ids, attention_mask=attMask, labels=targets)\n",
        "\n",
        "        if global_params['gradient_accumulation_steps'] > 1:\n",
        "            loss = loss/global_params['gradient_accumulation_steps']\n",
        "        loss.backward()\n",
        "\n",
        "        temp_loss += loss.item()\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            prec, a, b = precision(logits, targets)\n",
        "            print(\"epoch: {}\\t| Cnt: {}\\t| Samples: {}, Loss: {}\\t| precision: {}\".format(\n",
        "                e, cnt, nb_tr_examples, temp_loss/100, prec))\n",
        "            temp_loss = 0\n",
        "\n",
        "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "\n",
        "def evaluation():\n",
        "    model.eval()\n",
        "\n",
        "    y = []\n",
        "    y_label = []\n",
        "    tr_loss = 0\n",
        "    for step, batch in enumerate(testload):\n",
        "        model.eval()\n",
        "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
        "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
        "\n",
        "        age_ids = age_ids.to(global_params['device'])\n",
        "        input_ids = input_ids.to(global_params['device'])\n",
        "        posi_ids = posi_ids.to(global_params['device'])\n",
        "        segment_ids = segment_ids.to(global_params['device'])\n",
        "        attMask = attMask.to(global_params['device'])\n",
        "        targets = targets.to(global_params['device'])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss, logits = model(input_ids, age_ids, segment_ids,\n",
        "                                 posi_ids, attention_mask=attMask, labels=targets)\n",
        "        logits = logits.cpu()\n",
        "        targets = targets.cpu()\n",
        "\n",
        "        y_label.append(targets)\n",
        "        y.append(logits)\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "    y_label = torch.cat(y_label, dim=0)\n",
        "    y = torch.cat(y, dim=0)\n",
        "\n",
        "    aps, roc, output, label = precision_test(y, y_label)\n",
        "    return aps, roc, tr_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpft0UBsQPrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635bebcc-3169-4b36-de58-bf0b641593b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 training started...\n",
            "epoch: 0\t| Cnt: 1\t| Samples: 128, Loss: 0.0003302598744630814\t| precision: 0.49488489582218853\n",
            "epoch: 0\t| Cnt: 101\t| Samples: 12928, Loss: 0.03439836429432035\t| precision: 0.48220981917850014\n",
            "epoch: 0\t| Cnt: 201\t| Samples: 25728, Loss: 0.03403788104653358\t| precision: 0.478597313332621\n",
            "precision : 0.45311647406005195, auroc: 0.815636898862504,\n",
            "Epoch: 1 training started...\n",
            "epoch: 1\t| Cnt: 1\t| Samples: 128, Loss: 0.00035497024655342104\t| precision: 0.48953100901862845\n",
            "epoch: 1\t| Cnt: 101\t| Samples: 12928, Loss: 0.03425452115014196\t| precision: 0.4727963490205479\n",
            "epoch: 1\t| Cnt: 201\t| Samples: 25728, Loss: 0.03430703034624458\t| precision: 0.4770563058101994\n",
            "precision : 0.45175976549582736, auroc: 0.8157377769622934,\n",
            "Epoch: 2 training started...\n",
            "epoch: 2\t| Cnt: 1\t| Samples: 128, Loss: 0.00034146808087825775\t| precision: 0.48558854960753084\n",
            "epoch: 2\t| Cnt: 101\t| Samples: 12928, Loss: 0.034115400649607185\t| precision: 0.4946619451319671\n",
            "epoch: 2\t| Cnt: 201\t| Samples: 25728, Loss: 0.03434196563437581\t| precision: 0.47841630262234164\n",
            "precision : 0.4528687214296085, auroc: 0.816053563839835,\n",
            "Epoch: 3 training started...\n",
            "epoch: 3\t| Cnt: 1\t| Samples: 128, Loss: 0.0003590584546327591\t| precision: 0.45806101003317595\n",
            "epoch: 3\t| Cnt: 101\t| Samples: 12928, Loss: 0.034254719819873575\t| precision: 0.4819508604622478\n",
            "epoch: 3\t| Cnt: 201\t| Samples: 25728, Loss: 0.034089570101350546\t| precision: 0.4696438150740899\n",
            "precision : 0.4529730353449268, auroc: 0.8160240690575965,\n",
            "Epoch: 4 training started...\n",
            "epoch: 4\t| Cnt: 1\t| Samples: 128, Loss: 0.00032647117972373963\t| precision: 0.4949213029586068\n",
            "epoch: 4\t| Cnt: 101\t| Samples: 12928, Loss: 0.034048273134976624\t| precision: 0.47900391356369076\n",
            "epoch: 4\t| Cnt: 201\t| Samples: 25728, Loss: 0.03424008464440703\t| precision: 0.4967779616142839\n",
            "precision : 0.4539870351548613, auroc: 0.8162120763296119,\n",
            "Epoch: 5 training started...\n",
            "epoch: 5\t| Cnt: 1\t| Samples: 128, Loss: 0.00035138212144374847\t| precision: 0.4824760847314611\n",
            "epoch: 5\t| Cnt: 101\t| Samples: 12928, Loss: 0.03410578003153205\t| precision: 0.4779414259799365\n",
            "epoch: 5\t| Cnt: 201\t| Samples: 25728, Loss: 0.034177403207868336\t| precision: 0.493935780157779\n",
            "precision : 0.45239656203632417, auroc: 0.8163063466923549,\n",
            "Epoch: 6 training started...\n",
            "epoch: 6\t| Cnt: 1\t| Samples: 128, Loss: 0.0003566896915435791\t| precision: 0.4647953928597697\n",
            "epoch: 6\t| Cnt: 101\t| Samples: 12928, Loss: 0.0344173676520586\t| precision: 0.5038123906959115\n",
            "epoch: 6\t| Cnt: 201\t| Samples: 25728, Loss: 0.034186920560896394\t| precision: 0.4798522516558641\n",
            "precision : 0.4531037123441591, auroc: 0.8163995639375258,\n",
            "Epoch: 7 training started...\n",
            "epoch: 7\t| Cnt: 1\t| Samples: 128, Loss: 0.00033099479973316195\t| precision: 0.49618802057941813\n",
            "epoch: 7\t| Cnt: 101\t| Samples: 12928, Loss: 0.03414200954139233\t| precision: 0.48134959967335417\n",
            "epoch: 7\t| Cnt: 201\t| Samples: 25728, Loss: 0.034327438194304706\t| precision: 0.4998593339669381\n",
            "precision : 0.45303856808446963, auroc: 0.8160499603323257,\n",
            "Epoch: 8 training started...\n",
            "epoch: 8\t| Cnt: 1\t| Samples: 128, Loss: 0.0003364633023738861\t| precision: 0.49071469829863523\n",
            "epoch: 8\t| Cnt: 101\t| Samples: 12928, Loss: 0.03409986473619938\t| precision: 0.5078479211958331\n",
            "epoch: 8\t| Cnt: 201\t| Samples: 25728, Loss: 0.03417688261717558\t| precision: 0.4846068456952273\n",
            "precision : 0.4532216753710347, auroc: 0.8162797334096014,\n",
            "Epoch: 9 training started...\n",
            "epoch: 9\t| Cnt: 1\t| Samples: 128, Loss: 0.00034613221883773805\t| precision: 0.4755919074231668\n",
            "epoch: 9\t| Cnt: 101\t| Samples: 12928, Loss: 0.034458701461553574\t| precision: 0.47333734024652063\n",
            "epoch: 9\t| Cnt: 201\t| Samples: 25728, Loss: 0.03429368905723095\t| precision: 0.48099287211520025\n",
            "precision : 0.4524104016403407, auroc: 0.8162619770414937,\n",
            "Epoch: 10 training started...\n",
            "epoch: 10\t| Cnt: 1\t| Samples: 128, Loss: 0.0003466833382844925\t| precision: 0.4730556131151626\n",
            "epoch: 10\t| Cnt: 101\t| Samples: 12928, Loss: 0.034216574616730215\t| precision: 0.49484427042152057\n",
            "epoch: 10\t| Cnt: 201\t| Samples: 25728, Loss: 0.03393692471086979\t| precision: 0.4966180604000961\n",
            "precision : 0.4531632123032301, auroc: 0.8162506450956873,\n",
            "Epoch: 11 training started...\n",
            "epoch: 11\t| Cnt: 1\t| Samples: 128, Loss: 0.00034111194312572477\t| precision: 0.49279782588231513\n",
            "epoch: 11\t| Cnt: 101\t| Samples: 12928, Loss: 0.03396449996158481\t| precision: 0.4883885815659208\n",
            "epoch: 11\t| Cnt: 201\t| Samples: 25728, Loss: 0.034162758979946375\t| precision: 0.4942928659530901\n",
            "precision : 0.4549299678222114, auroc: 0.8166269625792538,\n",
            "Epoch: 12 training started...\n",
            "epoch: 12\t| Cnt: 1\t| Samples: 128, Loss: 0.00033757928758859634\t| precision: 0.4816189885561819\n",
            "epoch: 12\t| Cnt: 101\t| Samples: 12928, Loss: 0.034077461995184424\t| precision: 0.4845067180873822\n",
            "epoch: 12\t| Cnt: 201\t| Samples: 25728, Loss: 0.0339963641948998\t| precision: 0.49265666211643294\n",
            "precision : 0.458962783058283, auroc: 0.8187081440844457,\n",
            "Epoch: 13 training started...\n",
            "epoch: 13\t| Cnt: 1\t| Samples: 128, Loss: 0.0003455018624663353\t| precision: 0.4890264605188188\n",
            "epoch: 13\t| Cnt: 101\t| Samples: 12928, Loss: 0.03388420354574919\t| precision: 0.4872422001026683\n",
            "epoch: 13\t| Cnt: 201\t| Samples: 25728, Loss: 0.0337730261310935\t| precision: 0.51378580658897\n",
            "precision : 0.46363542061741997, auroc: 0.8227821119621485,\n",
            "Epoch: 14 training started...\n",
            "epoch: 14\t| Cnt: 1\t| Samples: 128, Loss: 0.00031502436846494677\t| precision: 0.5189389177145123\n",
            "epoch: 14\t| Cnt: 101\t| Samples: 12928, Loss: 0.033503209706395864\t| precision: 0.504225290243951\n",
            "epoch: 14\t| Cnt: 201\t| Samples: 25728, Loss: 0.03378662144765258\t| precision: 0.49473680484531923\n",
            "precision : 0.4693982351766986, auroc: 0.8269080885036763,\n",
            "Epoch: 15 training started...\n",
            "epoch: 15\t| Cnt: 1\t| Samples: 128, Loss: 0.0003181397169828415\t| precision: 0.5123365874313418\n",
            "epoch: 15\t| Cnt: 101\t| Samples: 12928, Loss: 0.033414139989763496\t| precision: 0.5180690304178177\n",
            "epoch: 15\t| Cnt: 201\t| Samples: 25728, Loss: 0.03352638298645615\t| precision: 0.5075903839824074\n",
            "precision : 0.47612049851107874, auroc: 0.8328691307864841,\n",
            "Epoch: 16 training started...\n",
            "epoch: 16\t| Cnt: 1\t| Samples: 128, Loss: 0.0003715162351727486\t| precision: 0.5039910437711489\n",
            "epoch: 16\t| Cnt: 101\t| Samples: 12928, Loss: 0.03316813055425882\t| precision: 0.5162276105224558\n",
            "epoch: 16\t| Cnt: 201\t| Samples: 25728, Loss: 0.033012664634734394\t| precision: 0.5248500485191218\n",
            "precision : 0.481529445804981, auroc: 0.8394596071807492,\n",
            "Epoch: 17 training started...\n",
            "epoch: 17\t| Cnt: 1\t| Samples: 128, Loss: 0.00032601069658994675\t| precision: 0.5005867314380087\n",
            "epoch: 17\t| Cnt: 101\t| Samples: 12928, Loss: 0.03277796035632491\t| precision: 0.5429476882731666\n",
            "epoch: 17\t| Cnt: 201\t| Samples: 25728, Loss: 0.032579057049006224\t| precision: 0.5205371775796014\n",
            "precision : 0.490066261737092, auroc: 0.8505022896308648,\n",
            "Epoch: 18 training started...\n",
            "epoch: 18\t| Cnt: 1\t| Samples: 128, Loss: 0.0003345450386404991\t| precision: 0.522420671965906\n",
            "epoch: 18\t| Cnt: 101\t| Samples: 12928, Loss: 0.03229024549946189\t| precision: 0.5566967953355727\n",
            "epoch: 18\t| Cnt: 201\t| Samples: 25728, Loss: 0.03218525428324938\t| precision: 0.4999449528379954\n",
            "precision : 0.4982829586680633, auroc: 0.8618469390488279,\n",
            "Epoch: 19 training started...\n",
            "epoch: 19\t| Cnt: 1\t| Samples: 128, Loss: 0.00033434830605983735\t| precision: 0.5160082494608116\n",
            "epoch: 19\t| Cnt: 101\t| Samples: 12928, Loss: 0.032014222480356694\t| precision: 0.5309861785504421\n",
            "epoch: 19\t| Cnt: 201\t| Samples: 25728, Loss: 0.03151282941922545\t| precision: 0.5459582344077318\n",
            "precision : 0.5078713394953799, auroc: 0.8737046610269813,\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "optim_config = {\n",
        "    'lr': optim_config['lr'],\n",
        "    'warmup_proportion': 0.1\n",
        "}\n",
        "optim = optimizer.adam(params=list(\n",
        "    model.named_parameters()), config=optim_config)\n",
        "\n",
        "best_pre = 0.512\n",
        "for e in range(global_params['num_epochs']):\n",
        "    print(f\"Epoch: {e} training started...\")\n",
        "    train(e)\n",
        "\n",
        "    aps, roc, test_loss = evaluation()\n",
        "    if aps > best_pre:\n",
        "        # Save a trained model\n",
        "        print(\"** ** * Saving fine - tuned model ** ** * \")\n",
        "        model_to_save = model.module if hasattr(\n",
        "            model, 'module') else model  # Only save the model it-self\n",
        "        output_model_file = os.path.join(global_params['output_dir'], global_params['best_name'])\n",
        "        \n",
        "        utils.create_folder(global_params['output_dir'])\n",
        "        if global_params['save_model']:\n",
        "            torch.save(model_to_save.state_dict(), output_model_file)\n",
        "        best_pre = aps\n",
        "\n",
        "    print('precision : {}, auroc: {},'.format(aps, roc))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}