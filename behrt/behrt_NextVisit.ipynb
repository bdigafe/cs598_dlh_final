{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDIyndGVBJZh"
      },
      "source": [
        "### Task 3: Next 6-months prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtTLL2DrBJZk"
      },
      "source": [
        "#### Fine tunning\n",
        "\n",
        "BERT generates word embeddings for the vocabulary words included in the corpus. In other words, each word or medical diganoses in our case, is mapped to a vector. In contrast to Word2Vec, BERT generates code that captures the local context of words and therefore provides a better representation of the word.\n",
        "\n",
        "MLM (Masked Language Model) is used for optimization, which we already performed on the 2nd task. In this step, we will add a task to predict the diagnosis codes after 6-months from a randomly selected visit date. This new task will be trained and the word embeddings will be fine tuned as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dnOQmjBmQPrk"
      },
      "outputs": [],
      "source": [
        "local_mode = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCUA0c8cBgHD",
        "outputId": "31e1a168-d810-4fb4-99f2-577561513178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-07 20:01:59--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/commons/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12014 (12K) [text/plain]\n",
            "Saving to: ‘commons/utils.py’\n",
            "\n",
            "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]  11.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-07 20:02:00 (86.4 MB/s) - ‘commons/utils.py’ saved [12014/12014]\n",
            "\n",
            "--2023-05-07 20:02:00--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/commons/__init__.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 0 [text/plain]\n",
            "Saving to: ‘commons/__init__.py’\n",
            "\n",
            "__init__.py             [ <=>                ]       0  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-07 20:02:00 (0.00 B/s) - ‘commons/__init__.py’ saved [0/0]\n",
            "\n",
            "--2023-05-07 20:02:00--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/MLM.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6199 (6.1K) [text/plain]\n",
            "Saving to: ‘models/MLM.py’\n",
            "\n",
            "MLM.py              100%[===================>]   6.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-07 20:02:00 (72.1 MB/s) - ‘models/MLM.py’ saved [6199/6199]\n",
            "\n",
            "--2023-05-07 20:02:00--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/NextXVisit.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6247 (6.1K) [text/plain]\n",
            "Saving to: ‘models/NextXVisit.py’\n",
            "\n",
            "NextXVisit.py       100%[===================>]   6.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-07 20:02:00 (75.6 MB/s) - ‘models/NextXVisit.py’ saved [6247/6247]\n",
            "\n",
            "--2023-05-07 20:02:01--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/optimizer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 772 [text/plain]\n",
            "Saving to: ‘models/optimizer.py’\n",
            "\n",
            "optimizer.py        100%[===================>]     772  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-07 20:02:01 (45.7 MB/s) - ‘models/optimizer.py’ saved [772/772]\n",
            "\n",
            "--2023-05-07 20:02:01--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/BertConfig.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 953 [text/plain]\n",
            "Saving to: ‘models/BertConfig.py’\n",
            "\n",
            "BertConfig.py       100%[===================>]     953  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-07 20:02:01 (39.7 MB/s) - ‘models/BertConfig.py’ saved [953/953]\n",
            "\n",
            "--2023-05-07 20:02:01--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/__init__.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 0 [text/plain]\n",
            "Saving to: ‘models/__init__.py’\n",
            "\n",
            "__init__.py             [ <=>                ]       0  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-07 20:02:01 (0.00 B/s) - ‘models/__init__.py’ saved [0/0]\n",
            "\n",
            "--2023-05-07 20:02:01--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/ages.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12929552 (12M) [application/octet-stream]\n",
            "Saving to: ‘data/ages.pkl’\n",
            "\n",
            "ages.pkl            100%[===================>]  12.33M  62.4MB/s    in 0.2s    \n",
            "\n",
            "2023-05-07 20:02:02 (62.4 MB/s) - ‘data/ages.pkl’ saved [12929552/12929552]\n",
            "\n",
            "--2023-05-07 20:02:02--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/concept.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15122701 (14M) [application/octet-stream]\n",
            "Saving to: ‘data/concept.pkl’\n",
            "\n",
            "concept.pkl         100%[===================>]  14.42M  77.9MB/s    in 0.2s    \n",
            "\n",
            "2023-05-07 20:02:02 (77.9 MB/s) - ‘data/concept.pkl’ saved [15122701/15122701]\n",
            "\n",
            "--2023-05-07 20:02:02--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/condition_codes.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18889 (18K) [application/octet-stream]\n",
            "Saving to: ‘data/condition_codes.pkl’\n",
            "\n",
            "condition_codes.pkl 100%[===================>]  18.45K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-05-07 20:02:02 (23.9 MB/s) - ‘data/condition_codes.pkl’ saved [18889/18889]\n",
            "\n",
            "--2023-05-07 20:02:02--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/conditions.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10038026 (9.6M) [application/octet-stream]\n",
            "Saving to: ‘data/conditions.pkl’\n",
            "\n",
            "conditions.pkl      100%[===================>]   9.57M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-07 20:02:03 (81.8 MB/s) - ‘data/conditions.pkl’ saved [10038026/10038026]\n",
            "\n",
            "--2023-05-07 20:02:03--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/saved_models/mlm128.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17990392 (17M) [application/octet-stream]\n",
            "Saving to: ‘saved_models/mlm128.pt’\n",
            "\n",
            "mlm128.pt           100%[===================>]  17.16M   109MB/s    in 0.2s    \n",
            "\n",
            "2023-05-07 20:02:03 (109 MB/s) - ‘saved_models/mlm128.pt’ saved [17990392/17990392]\n",
            "\n",
            "--2023-05-07 20:02:03--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/cdm54.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 912664 (891K) [image/png]\n",
            "Saving to: ‘images/cdm54.png’\n",
            "\n",
            "cdm54.png           100%[===================>] 891.27K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-05-07 20:02:04 (12.4 MB/s) - ‘images/cdm54.png’ saved [912664/912664]\n",
            "\n",
            "--2023-05-07 20:02:04--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/behrt_embeddings.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 450527 (440K) [image/png]\n",
            "Saving to: ‘images/behrt_embeddings.png’\n",
            "\n",
            "behrt_embeddings.pn 100%[===================>] 439.97K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-05-07 20:02:04 (12.6 MB/s) - ‘images/behrt_embeddings.png’ saved [450527/450527]\n",
            "\n",
            "--2023-05-07 20:02:04--  https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/behrt_model.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 116319 (114K) [image/png]\n",
            "Saving to: ‘images/behrt_model.png’\n",
            "\n",
            "behrt_model.png     100%[===================>] 113.59K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-05-07 20:02:04 (5.24 MB/s) - ‘images/behrt_model.png’ saved [116319/116319]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if not local_mode:\n",
        "  !mkdir commons\n",
        "  !wget -P commons https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/commons/utils.py\n",
        "  !wget -P commons https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/commons/__init__.py\n",
        "\n",
        "  !mkdir models\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/MLM.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/NextXVisit.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/optimizer.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/BertConfig.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/__init__.py\n",
        "\n",
        "  !mkdir data\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/ages.pkl\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/concept.pkl\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/condition_codes.pkl\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/conditions.pkl\n",
        "\n",
        "  !mkdir saved_models\n",
        "  !wget -P saved_models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/saved_models/mlm128.pt\n",
        "\n",
        "  !mkdir images\n",
        "  !wget -P images https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/cdm54.png\n",
        "  !wget -P images https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/behrt_embeddings.png\n",
        "  !wget -P images https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/behrt_model.png\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not local_mode:\n",
        "  %pip install pytorch_pretrained_bert "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWpwpgYeZGJg",
        "outputId": "14cda4bb-d5ec-4d37-8725-8ab4f41dfd10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.27.1)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.129-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.65.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (3.25.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.129\n",
            "  Downloading botocore-1.29.129-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.129->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.129->boto3->pytorch_pretrained_bert) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch_pretrained_bert\n",
            "Successfully installed boto3-1.26.129 botocore-1.29.129 jmespath-1.0.1 pytorch_pretrained_bert-0.6.2 s3transfer-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXiIjJdbQPrl"
      },
      "source": [
        "### File and model paramters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IayjuaWgBJZm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import commons.utils as utils\n",
        "from models import optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_pretrained_bert as Bert\n",
        "\n",
        "# set random seed for reproducibility\n",
        "seed = 123\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ['PYTHONASHSEED'] = str(seed)\n",
        "\n",
        "global_params = {\n",
        "    'max_seq_len': 256,\n",
        "    'max_age' : 110,\n",
        "    'age_month' : 1,\n",
        "    'batch_size': 128,\n",
        "    'num_epochs': 20,\n",
        "    'min_visit': 5,\n",
        "    'gradient_accumulation_steps': 1,\n",
        "    'training_sample' : 0,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "}\n",
        "\n",
        "optim_config = {\n",
        "    'lr': 3e-5,\n",
        "    'warmup_proportion': 0.1,\n",
        "    'weight_decay': 0.01\n",
        "}\n",
        "\n",
        "file_config = {\n",
        "    'vocab': ('C:/Birhanu/Education/UIL/cs598/Final/data/' if local_mode else 'data/') + 'condition_codes.pkl',\n",
        "    'data': ('C:/Birhanu/Education/UIL/cs598/Final/data/' if local_mode else 'data/' ) + 'conditions.pkl',\n",
        "    'ages' : ('C:/Birhanu/Education/UIL/cs598/Final/data/' if local_mode else 'data/' ) + 'ages.pkl',\n",
        "\n",
        "    'model_path': 'C:/Birhanu/Education/UIL/cs598/Final/saved_models/' if local_mode else 'saved_models/', \n",
        "    'model_name': 'next6m-model',  # model name\n",
        "    \n",
        "    'log_file_name': 'next6m-log',  # log path\n",
        "}\n",
        "\n",
        "# MLM pretrained model path\n",
        "if local_mode:\n",
        "  pretrainModel = 'C:/Birhanu/Education/UIL/cs598/Final/saved_models/mlm128.pt'\n",
        "else:\n",
        "   pretrainModel = 'saved_models/mlm128.pt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AkM2z7YqH0K6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9248cf7-d5a9-49e6-c15e-2cc97f843b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "print(global_params[\"device\"])\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt_kVGeZBJZo"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf_4wsLcBJZo",
        "outputId": "b916d7fa-b3d9-4da7-9f76-a322f90b5a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conditions vocab file: data/condition_codes.pkl\n",
            "      pid                                              visit  \\\n",
            "0  176102  [35207924, 35211387, SEP, 1569634, SEP, 157606...   \n",
            "1  176104  [35208969, SEP, 35208968, 35208969, SEP, 35208...   \n",
            "2  176106  [35207966, SEP, 45591555, SEP, 1570694, 352089...   \n",
            "3  176107  [1569446, SEP, 1571058, 1574193, 35209007, SEP...   \n",
            "4  176108  [35208190, SEP, 35208481, SEP, 1567866, SEP, 1...   \n",
            "\n",
            "                                                 age                     label  \n",
            "0  [230, 230, 230, 262, 262, 263, 263, 267, 267, ...            [1576063, SEP]  \n",
            "1  [623, 623, 623, 623, 623, 623, 623, 623, 625, ...   [1568344, 1572256, SEP]  \n",
            "2  [455, 455, 460, 460, 467, 467, 467, 467, 467, ...  [1570694, 35208968, SEP]  \n",
            "3  [407, 407, 408, 408, 408, 408, 409, 409, 413, ...            [1570790, SEP]  \n",
            "4  [779, 779, 784, 784, 795, 795, 796, 796, 797, ...   [1567750, 1569558, SEP]  \n"
          ]
        }
      ],
      "source": [
        "# Load conditions codes (word vocabs)\n",
        "print(f\"Conditions vocab file: {file_config['vocab']}\")\n",
        "vocab_conditions = utils.get_codes_vocab(file_config[\"vocab\"])\n",
        "code2idx = vocab_conditions[\"token2idx\"]\n",
        "#code2idx = utils.remove_system_codes_from_token_dict(code2idx)\n",
        "\n",
        "# Create age vocab\n",
        "vocab_age = utils.age_vocab(global_params[\"max_age\"], global_params[\"age_month\"])\n",
        "age2idx = vocab_age[0]\n",
        "\n",
        "# Load data\n",
        "data_conditions = utils.load_data(file_config[\"data\"], sample_size=global_params[\"training_sample\"])\n",
        "data_age_seqs = utils.load_data(file_config[\"ages\"], sample_size=global_params[\"training_sample\"])\n",
        "\n",
        "# Split data into train, validation, and test\n",
        "train_data, test_data = utils.split_data(data_conditions, data_age_seqs, train_ratio=0.8, min_size=3)\n",
        "print(train_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m-yUy_LjQPrn"
      },
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    # number of disease + symbols for word embedding\n",
        "    'vocab_size': len(vocab_conditions['token2idx'].keys()),\n",
        "\n",
        "    # word embedding and seg embedding hidden size\n",
        "    'hidden_size': 288, \n",
        "\n",
        "    # number of vocab for seg embedding\n",
        "    'seg_vocab_size': 2, \n",
        "\n",
        "    # number of vocab for age embedding\n",
        "    'age_vocab_size': len(vocab_age[0].keys()),\n",
        "\n",
        "    # maximum number of tokens\n",
        "    'max_position_embedding': global_params['max_seq_len'],\n",
        "\n",
        "    # dropout rate\n",
        "    'hidden_dropout_prob': 0.2, \n",
        "\n",
        "    # number of multi-head attention layers required\n",
        "    'num_hidden_layers': 6,  \n",
        "\n",
        "    # number of attention heads\n",
        "    'num_attention_heads': 12,\n",
        "\n",
        "    # multi-head attention dropout rate  \n",
        "    'attention_probs_dropout_prob': 0.22,  \n",
        "\n",
        "    # the size of the \"intermediate\" layer in the transformer encoder\n",
        "    'intermediate_size': 512,\n",
        "\n",
        "    # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
        "    'hidden_act': 'gelu',\n",
        "    'initializer_range': 0.02,  # parameter weight initializer range\n",
        "}\n",
        "\n",
        "feature_dict = {\n",
        "    'age': True,\n",
        "    'seg': True,\n",
        "    'posi': True\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "smgL5LjrQPrn"
      },
      "outputs": [],
      "source": [
        "class NextVisit(Dataset):\n",
        "    def __init__(self, token2idx, diag2idx, age2idx, dataframe, max_len, max_age=110, min_visit=5):\n",
        "        # dataframe preproecssing\n",
        "        # filter out the patient with number of visits less than min_visit\n",
        "        self.vocab = token2idx\n",
        "        self.label_vocab = diag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.code = dataframe.visit\n",
        "        self.age = dataframe.age\n",
        "        self.label = dataframe.label\n",
        "        self.patid = dataframe.pid\n",
        "\n",
        "        self.age2idx = age2idx\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        return: age, code, position, segmentation, mask, label\n",
        "        \"\"\"\n",
        "        # cut data\n",
        "        age = self.age[index]\n",
        "        code = self.code[index]\n",
        "        label = self.label[index]\n",
        "        patid = self.patid[index]\n",
        "\n",
        "        # extract data\n",
        "        age = age[(-self.max_len+1):]\n",
        "        code = code[(-self.max_len+1):]\n",
        "\n",
        "        # avoid data cut with first element to be 'SEP'\n",
        "        if code[0] != 'SEP':\n",
        "            code = np.append(np.array(['CLS']), code)\n",
        "            age = np.append(np.array(age[0]), age)\n",
        "        else:\n",
        "            code[0] = 'CLS'\n",
        "\n",
        "        # mask 0:len(code) to 1, padding to be 0\n",
        "        mask = np.ones(self.max_len)\n",
        "        mask[len(code):] = 0\n",
        "\n",
        "        # pad age sequence and code sequence\n",
        "        age = utils.seq_padding(age, self.max_len, token2idx=self.age2idx)\n",
        "\n",
        "        tokens, code = utils.code2index(code, self.vocab)\n",
        "        _, label = utils.code2index(label, self.label_vocab)\n",
        "\n",
        "        # get position code and segment code\n",
        "        tokens = utils.seq_padding(tokens, self.max_len)\n",
        "        position = utils.position_idx(tokens)\n",
        "        segment = utils.index_seg(tokens)\n",
        "\n",
        "        # pad code and label\n",
        "        code = utils.seq_padding(code, self.max_len, symbol=self.vocab['PAD'])\n",
        "        label = utils.seq_padding(label, self.max_len, symbol=-1)\n",
        "\n",
        "        return torch.LongTensor(age), \\\n",
        "              torch.LongTensor(code), \\\n",
        "              torch.LongTensor(position), \\\n",
        "              torch.LongTensor(segment), \\\n",
        "              torch.LongTensor(mask), \\\n",
        "              torch.LongTensor(label), \\\n",
        "              torch.LongTensor([int(patid)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.code)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test batch data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def inspect_batch():\n",
        "    dataset = NextVisit(token2idx=code2idx, diag2idx=code2idx,\n",
        "                 age2idx=age2idx, dataframe=train_data, \n",
        "                 max_len=global_params['max_seq_len'])\n",
        "    \n",
        "    train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)\n",
        "    loader_iter = iter(train_loader)\n",
        "    batch = next(loader_iter)\n",
        "\n",
        "    batch = tuple(t.to(global_params['device']) for t in batch)\n",
        "    age_ids, input_ids, posi_ids, segment_ids, attMask, label, masked_label = batch\n",
        "    \n",
        "    # Token codes\n",
        "    p = 15\n",
        "    print(f\"Input:\\n {input_ids}\\n\")\n",
        "    print(f\"Age:\\n{age_ids}\\n\")\n",
        "    print(f\"Positions:\\n{posi_ids}\\n\")\n",
        "    print(f\"Segments:\\n{segment_ids}\\n\")\n",
        "\n",
        "    print(f\"attMask: {attMask}\")\n",
        "    print(f\"Labels: {label}\")\n",
        "\n",
        "inspect_batch()\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKVoJMjZVSZ9",
        "outputId": "a2035953-67a1-4509-9743-457bb7b24727"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[  0, 238, 275,   1,  96,   1, 203,   1,  78, 163,   1,  80,   1,  76,\n",
            "           1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "           2,   2,   2,   2]], device='cuda:0')\n",
            "\n",
            "Age:\n",
            "tensor([[232, 232, 232, 232, 264, 264, 265, 265, 269, 269, 269, 270, 270, 270,\n",
            "         270,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0]], device='cuda:0')\n",
            "\n",
            "Positions:\n",
            "tensor([[0, 0, 0, 0, 1, 1, 2, 2, 3, 3, 3, 4, 4, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "         6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]], device='cuda:0')\n",
            "\n",
            "Segments:\n",
            "tensor([[0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
            "\n",
            "attMask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
            "Labels: tensor([[203,   1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "          -1,  -1,  -1,  -1]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a-b9og3rQPro"
      },
      "outputs": [],
      "source": [
        "class BertConfig(Bert.modeling.BertConfig):\n",
        "    def __init__(self, config):\n",
        "        super(BertConfig, self).__init__(\n",
        "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
        "            hidden_size=config['hidden_size'],\n",
        "            num_hidden_layers=config.get('num_hidden_layers'),\n",
        "            num_attention_heads=config.get('num_attention_heads'),\n",
        "            intermediate_size=config.get('intermediate_size'),\n",
        "            hidden_act=config.get('hidden_act'),\n",
        "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
        "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
        "            max_position_embeddings=config.get('max_position_embedding'),\n",
        "            initializer_range=config.get('initializer_range'),\n",
        "        )\n",
        "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
        "        self.age_vocab_size = config.get('age_vocab_size')\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, segment, age\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, feature_dict):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.feature_dict = feature_dict\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(\n",
        "            config.vocab_size, config.hidden_size)\n",
        "        self.segment_embeddings = nn.Embedding(\n",
        "            config.seg_vocab_size, config.hidden_size)\n",
        "        self.age_embeddings = nn.Embedding(\n",
        "            config.age_vocab_size, config.hidden_size)\n",
        "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
        "            from_pretrained(embeddings=self._init_posi_embedding(\n",
        "                config.max_position_embeddings, config.hidden_size))\n",
        "\n",
        "        self.LayerNorm = Bert.modeling.BertLayerNorm(\n",
        "            config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, word_ids, age_ids=None, seg_ids=None, posi_ids=None, age=True):\n",
        "        if seg_ids is None:\n",
        "            seg_ids = torch.zeros_like(word_ids)\n",
        "        if age_ids is None:\n",
        "            age_ids = torch.zeros_like(word_ids)\n",
        "        if posi_ids is None:\n",
        "            posi_ids = torch.zeros_like(word_ids)\n",
        "\n",
        "        word_embed = self.word_embeddings(word_ids)\n",
        "        segment_embed = self.segment_embeddings(seg_ids)\n",
        "        age_embed = self.age_embeddings(age_ids)\n",
        "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
        "\n",
        "        embeddings = word_embed\n",
        "\n",
        "        if self.feature_dict['age']:\n",
        "            embeddings = embeddings + age_embed\n",
        "        if self.feature_dict['seg']:\n",
        "            embeddings = embeddings + segment_embed\n",
        "        if self.feature_dict['posi']:\n",
        "            embeddings = embeddings + posi_embeddings\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
        "        def even_code(pos, idx):\n",
        "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
        "\n",
        "        def odd_code(pos, idx):\n",
        "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
        "\n",
        "        # initialize position embedding table\n",
        "        lookup_table = np.zeros(\n",
        "            (max_position_embedding, hidden_size), dtype=np.float32)\n",
        "\n",
        "        # reset table parameters with hard encoding\n",
        "        # set even dimension\n",
        "        for pos in range(max_position_embedding):\n",
        "            for idx in np.arange(0, hidden_size, step=2):\n",
        "                lookup_table[pos, idx] = even_code(pos, idx)\n",
        "        # set odd dimension\n",
        "        for pos in range(max_position_embedding):\n",
        "            for idx in np.arange(1, hidden_size, step=2):\n",
        "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
        "\n",
        "        return torch.tensor(lookup_table)\n",
        "\n",
        "\n",
        "class BertModel(Bert.modeling.BertPreTrainedModel):\n",
        "    def __init__(self, config, feature_dict):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config, feature_dict)\n",
        "        self.encoder = Bert.modeling.BertEncoder(config=config)\n",
        "        self.pooler = Bert.modeling.BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if age_ids is None:\n",
        "            age_ids = torch.zeros_like(input_ids)\n",
        "        if seg_ids is None:\n",
        "            seg_ids = torch.zeros_like(input_ids)\n",
        "        if posi_ids is None:\n",
        "            posi_ids = torch.zeros_like(input_ids)\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids, age_ids, seg_ids, posi_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output\n",
        "\n",
        "\n",
        "class BertForMultiLabelPrediction(Bert.modeling.BertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels, feature_dict):\n",
        "\n",
        "        super(BertForMultiLabelPrediction, self).__init__(config)\n",
        "        \n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config, feature_dict)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, age_ids, seg_ids, posi_ids, attention_mask,\n",
        "                                     output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.MultiLabelSoftMarginLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels),\n",
        "                            labels.view(-1, self.num_labels))\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJbUpHkUQPro"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EkOCTrpNQPro"
      },
      "outputs": [],
      "source": [
        "Dset = NextVisit(token2idx=code2idx, diag2idx=code2idx,\n",
        "                 age2idx=age2idx, dataframe=train_data, \n",
        "                 max_len=global_params['max_seq_len'])\n",
        "\n",
        "trainload = DataLoader(\n",
        "    dataset=Dset, batch_size=global_params['batch_size'], shuffle=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6_9fGilgQPro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4dd473-ebef-4abc-d871-61635643df5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "Dset = NextVisit(token2idx=code2idx, diag2idx=code2idx,\n",
        "                 age2idx=age2idx, dataframe=test_data, max_len=global_params['max_seq_len'])\n",
        "\n",
        "testload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=False, num_workers=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDLMK-y_QPrp"
      },
      "source": [
        "### Setup Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8FqEkVMQPrp",
        "outputId": "724201f5-c884-487a-d6c9-828dd6ec2a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of conditions vocab: 301\n",
            "Size of Age vocab: 1322\n"
          ]
        }
      ],
      "source": [
        "print(f\"Size of conditions vocab: {len(vocab_conditions['token2idx'])}\")\n",
        "print(f\"Size of Age vocab: {len(vocab_age[0].keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T3UBaFLSBJZz"
      },
      "outputs": [],
      "source": [
        "conf = BertConfig(model_config)\n",
        "model = BertForMultiLabelPrediction(conf,  len(vocab_conditions[\"token2idx\"]), feature_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbXmFCpWqOYW",
        "outputId": "ea2880e6-1e5d-4196-cd0c-a38c96a457fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMultiLabelPrediction(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(301, 288)\n",
              "      (segment_embeddings): Embedding(2, 288)\n",
              "      (age_embeddings): Embedding(1322, 288)\n",
              "      (posi_embeddings): Embedding(256, 288)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (key): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (value): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (dropout): Dropout(p=0.22, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=288, out_features=512, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=512, out_features=288, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=288, out_features=288, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (classifier): Linear(in_features=288, out_features=301, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdFJ-lfOQPrp",
        "outputId": "dd78d00e-7323-4001-81b2-c62a05017fec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# load pretrained model and update weights\n",
        "pretrained_dict = torch.load(pretrainModel)\n",
        "model_dict = model.state_dict()\n",
        "\n",
        "# 1. filter out unnecessary keys\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "\n",
        "# 2. overwrite entries in the existing state dict\n",
        "model_dict.update(pretrained_dict)\n",
        "\n",
        "# 3. load the new state dict\n",
        "model.load_state_dict(model_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LRdWVqJuQPrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6614de69-e523-4f22-b6a1-706474448ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n"
          ]
        }
      ],
      "source": [
        "model = model.to(global_params['device'])\n",
        "optim = optimizer.adam(params=list(\n",
        "    model.named_parameters()), config=optim_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4KnzqSUQPrq"
      },
      "source": [
        "### Evaluation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qnaTVcG5QPrq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics._plot.roc_curve import RocCurveDisplay\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def precision(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output = sig(logits)\n",
        "    label, output = label.cpu(), output.detach().cpu()\n",
        "    tempprc=  average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return tempprc, output, label\n",
        "\n",
        "def precision_test(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output=sig(logits)\n",
        "    tempprc= average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    roc = roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return tempprc, roc, output, label,\n",
        "\n",
        "def auroc_test(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output=sig(logits)\n",
        "    tempprc= average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    roc = roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return roc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJdYiqy6QPrq"
      },
      "source": [
        "### Multi-hot Label Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LjqNifovQPrq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "7b36a4d9-b82e-49ca-d935-b1e2745c092a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiLabelBinarizer(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n",
              "                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
              "                             28, 29, ...])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n",
              "                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
              "                             28, 29, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer(classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n",
              "                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
              "                             28, 29, ...])</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "indexes = list(vocab_conditions[\"token2idx\"].values())\n",
        "\n",
        "mlb = MultiLabelBinarizer(classes=list(indexes))\n",
        "mlb.fit([[each] for each in list(indexes)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZG7RjlTQPrq"
      },
      "source": [
        "### Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TjKqYlcAQPrq"
      },
      "outputs": [],
      "source": [
        "def train(e):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    temp_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    cnt = 0\n",
        "    for step, batch in enumerate(trainload):\n",
        "        cnt += 1\n",
        "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
        "        targets = torch.tensor(mlb.transform(\n",
        "            targets.numpy()), dtype=torch.float32)\n",
        "\n",
        "        age_ids = age_ids.to(global_params['device'])\n",
        "        input_ids = input_ids.to(global_params['device'])\n",
        "        posi_ids = posi_ids.to(global_params['device'])\n",
        "        segment_ids = segment_ids.to(global_params['device'])\n",
        "        attMask = attMask.to(global_params['device'])\n",
        "        targets = targets.to(global_params['device'])\n",
        "\n",
        "        loss, logits = model(input_ids, age_ids, segment_ids,\n",
        "                             posi_ids, attention_mask=attMask, labels=targets)\n",
        "\n",
        "        if global_params['gradient_accumulation_steps'] > 1:\n",
        "            loss = loss/global_params['gradient_accumulation_steps']\n",
        "        loss.backward()\n",
        "\n",
        "        temp_loss += loss.item()\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            prec, a, b = precision(logits, targets)\n",
        "            print(\"epoch: {}\\t| Cnt: {}\\t| Samples: {}, Loss: {}\\t| precision: {}\".format(\n",
        "                e, cnt, nb_tr_examples, temp_loss/100, prec))\n",
        "            temp_loss = 0\n",
        "\n",
        "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "\n",
        "def evaluation():\n",
        "    model.eval()\n",
        "\n",
        "    y = []\n",
        "    y_label = []\n",
        "    tr_loss = 0\n",
        "    for step, batch in enumerate(testload):\n",
        "        model.eval()\n",
        "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
        "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
        "\n",
        "        age_ids = age_ids.to(global_params['device'])\n",
        "        input_ids = input_ids.to(global_params['device'])\n",
        "        posi_ids = posi_ids.to(global_params['device'])\n",
        "        segment_ids = segment_ids.to(global_params['device'])\n",
        "        attMask = attMask.to(global_params['device'])\n",
        "        targets = targets.to(global_params['device'])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss, logits = model(input_ids, age_ids, segment_ids,\n",
        "                                 posi_ids, attention_mask=attMask, labels=targets)\n",
        "        logits = logits.cpu()\n",
        "        targets = targets.cpu()\n",
        "\n",
        "        y_label.append(targets)\n",
        "        y.append(logits)\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "    y_label = torch.cat(y_label, dim=0)\n",
        "    y = torch.cat(y, dim=0)\n",
        "\n",
        "    aps, roc, output, label = precision_test(y, y_label)\n",
        "    return aps, roc, tr_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bpft0UBsQPrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86cf3d9d-190e-4b72-e925-67ad5c69c66c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 training started...\n",
            "epoch: 0\t| Cnt: 1\t| Samples: 128, Loss: 0.00032580167055130005\t| precision: 0.4951861094891645\n",
            "epoch: 0\t| Cnt: 101\t| Samples: 12928, Loss: 0.03409361120313406\t| precision: 0.47717410519585807\n",
            "epoch: 0\t| Cnt: 201\t| Samples: 25728, Loss: 0.03428471818566323\t| precision: 0.4977926426765117\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.45390129810773905, auroc: 0.8160989197458903,\n",
            "Epoch: 0 completed. \t time: 238.1491026878357\n",
            "Epoch: 1 training started...\n",
            "epoch: 1\t| Cnt: 1\t| Samples: 128, Loss: 0.00034789972007274626\t| precision: 0.4858256158930965\n",
            "epoch: 1\t| Cnt: 101\t| Samples: 12928, Loss: 0.034129996858537195\t| precision: 0.47621042931785906\n",
            "epoch: 1\t| Cnt: 201\t| Samples: 25728, Loss: 0.034178324081003665\t| precision: 0.49077017906897935\n",
            "precision : 0.45234004678141715, auroc: 0.816310916266966,\n",
            "Epoch: 1 completed. \t time: 237.31621980667114\n",
            "Epoch: 2 training started...\n",
            "epoch: 2\t| Cnt: 1\t| Samples: 128, Loss: 0.0003590239584445953\t| precision: 0.46401455990683094\n",
            "epoch: 2\t| Cnt: 101\t| Samples: 12928, Loss: 0.034436841309070584\t| precision: 0.5030106446379704\n",
            "epoch: 2\t| Cnt: 201\t| Samples: 25728, Loss: 0.03416410708799958\t| precision: 0.4807195680930765\n",
            "precision : 0.4529420184256743, auroc: 0.8164673157129837,\n",
            "Epoch: 2 completed. \t time: 237.30290389060974\n",
            "Epoch: 3 training started...\n",
            "epoch: 3\t| Cnt: 1\t| Samples: 128, Loss: 0.0003314843028783798\t| precision: 0.49593891298254245\n",
            "epoch: 3\t| Cnt: 101\t| Samples: 12928, Loss: 0.034168260414153336\t| precision: 0.48387944284046164\n",
            "epoch: 3\t| Cnt: 201\t| Samples: 25728, Loss: 0.03432513250038028\t| precision: 0.49483298446920393\n",
            "precision : 0.4530168134645278, auroc: 0.816054661281007,\n",
            "Epoch: 3 completed. \t time: 238.1375470161438\n",
            "Epoch: 4 training started...\n",
            "epoch: 4\t| Cnt: 1\t| Samples: 128, Loss: 0.0003384749591350555\t| precision: 0.4893510930787563\n",
            "epoch: 4\t| Cnt: 101\t| Samples: 12928, Loss: 0.03408206276595593\t| precision: 0.5040481256908724\n",
            "epoch: 4\t| Cnt: 201\t| Samples: 25728, Loss: 0.03413742307573557\t| precision: 0.48601756873361984\n",
            "precision : 0.4532264514513131, auroc: 0.8162368317262212,\n",
            "Epoch: 4 completed. \t time: 237.97563886642456\n",
            "Epoch: 5 training started...\n",
            "epoch: 5\t| Cnt: 1\t| Samples: 128, Loss: 0.0003449629247188568\t| precision: 0.476139025111983\n",
            "epoch: 5\t| Cnt: 101\t| Samples: 12928, Loss: 0.03441614285111427\t| precision: 0.47852009549387187\n",
            "epoch: 5\t| Cnt: 201\t| Samples: 25728, Loss: 0.03422454824671149\t| precision: 0.4834441439853999\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.45419945120679, auroc: 0.8166103980692756,\n",
            "Epoch: 5 completed. \t time: 237.76259851455688\n",
            "Epoch: 6 training started...\n",
            "epoch: 6\t| Cnt: 1\t| Samples: 128, Loss: 0.00034401729702949525\t| precision: 0.47298083744147046\n",
            "epoch: 6\t| Cnt: 101\t| Samples: 12928, Loss: 0.034074777420610186\t| precision: 0.49640911640945806\n",
            "epoch: 6\t| Cnt: 201\t| Samples: 25728, Loss: 0.03374705854803324\t| precision: 0.4963123431580184\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.45988145052865603, auroc: 0.8190685520206539,\n",
            "Epoch: 6 completed. \t time: 237.78803396224976\n",
            "Epoch: 7 training started...\n",
            "epoch: 7\t| Cnt: 1\t| Samples: 128, Loss: 0.0003377348929643631\t| precision: 0.5013633778540156\n",
            "epoch: 7\t| Cnt: 101\t| Samples: 12928, Loss: 0.033716222178190945\t| precision: 0.49210451109095465\n",
            "epoch: 7\t| Cnt: 201\t| Samples: 25728, Loss: 0.03390105329453945\t| precision: 0.5000225626300703\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.4642124495090559, auroc: 0.8230372241795106,\n",
            "Epoch: 7 completed. \t time: 237.832510471344\n",
            "Epoch: 8 training started...\n",
            "epoch: 8\t| Cnt: 1\t| Samples: 128, Loss: 0.00033538423478603365\t| precision: 0.49018156731303353\n",
            "epoch: 8\t| Cnt: 101\t| Samples: 12928, Loss: 0.033737419024109844\t| precision: 0.4943252194979492\n",
            "epoch: 8\t| Cnt: 201\t| Samples: 25728, Loss: 0.03364396361634135\t| precision: 0.49971264514726443\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.470024788860156, auroc: 0.8274423082252076,\n",
            "Epoch: 8 completed. \t time: 238.06239199638367\n",
            "Epoch: 9 training started...\n",
            "epoch: 9\t| Cnt: 1\t| Samples: 128, Loss: 0.00034095220267772676\t| precision: 0.5020425581921654\n",
            "epoch: 9\t| Cnt: 101\t| Samples: 12928, Loss: 0.03349192405119538\t| precision: 0.5060499298722909\n",
            "epoch: 9\t| Cnt: 201\t| Samples: 25728, Loss: 0.03333186311647296\t| precision: 0.5234192213802331\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.4771183626566186, auroc: 0.8347416730156488,\n",
            "Epoch: 9 completed. \t time: 237.72150015830994\n",
            "Epoch: 10 training started...\n",
            "epoch: 10\t| Cnt: 1\t| Samples: 128, Loss: 0.0003118626587092876\t| precision: 0.5287835932478168\n",
            "epoch: 10\t| Cnt: 101\t| Samples: 12928, Loss: 0.03299278138205409\t| precision: 0.5213136930610595\n",
            "epoch: 10\t| Cnt: 201\t| Samples: 25728, Loss: 0.03321992252022028\t| precision: 0.5036103807205244\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.48250079390293177, auroc: 0.8428989491200267,\n",
            "Epoch: 10 completed. \t time: 238.27056574821472\n",
            "Epoch: 11 training started...\n",
            "epoch: 11\t| Cnt: 1\t| Samples: 128, Loss: 0.0003112155944108963\t| precision: 0.5289690309838284\n",
            "epoch: 11\t| Cnt: 101\t| Samples: 12928, Loss: 0.03275879180058837\t| precision: 0.5335536474639226\n",
            "epoch: 11\t| Cnt: 201\t| Samples: 25728, Loss: 0.03281576748937368\t| precision: 0.5171090054606986\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.48921439799643557, auroc: 0.85191790781632,\n",
            "Epoch: 11 completed. \t time: 237.93073201179504\n",
            "Epoch: 12 training started...\n",
            "epoch: 12\t| Cnt: 1\t| Samples: 128, Loss: 0.0003605908155441284\t| precision: 0.5144602808146499\n",
            "epoch: 12\t| Cnt: 101\t| Samples: 12928, Loss: 0.03231446344405413\t| precision: 0.5254146407980242\n",
            "epoch: 12\t| Cnt: 201\t| Samples: 25728, Loss: 0.03211677065119147\t| precision: 0.5332041678159601\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.49776162739938995, auroc: 0.8614947849149173,\n",
            "Epoch: 12 completed. \t time: 238.01511192321777\n",
            "Epoch: 13 training started...\n",
            "epoch: 13\t| Cnt: 1\t| Samples: 128, Loss: 0.00031807057559490203\t| precision: 0.515592563071996\n",
            "epoch: 13\t| Cnt: 101\t| Samples: 12928, Loss: 0.03179907606914639\t| precision: 0.5623970281080395\n",
            "epoch: 13\t| Cnt: 201\t| Samples: 25728, Loss: 0.031565692741423845\t| precision: 0.5348363425379161\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.5087142966649238, auroc: 0.8749023287174209,\n",
            "Epoch: 13 completed. \t time: 237.93247771263123\n",
            "Epoch: 14 training started...\n",
            "epoch: 14\t| Cnt: 1\t| Samples: 128, Loss: 0.0003223055973649025\t| precision: 0.5379334753598101\n",
            "epoch: 14\t| Cnt: 101\t| Samples: 12928, Loss: 0.03132091902196407\t| precision: 0.5718632989292081\n",
            "epoch: 14\t| Cnt: 201\t| Samples: 25728, Loss: 0.031216945089399813\t| precision: 0.5183316340078966\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.5158083576217085, auroc: 0.883207993119267,\n",
            "Epoch: 14 completed. \t time: 237.8751802444458\n",
            "Epoch: 15 training started...\n",
            "epoch: 15\t| Cnt: 1\t| Samples: 128, Loss: 0.0003208542242646217\t| precision: 0.5381905862119973\n",
            "epoch: 15\t| Cnt: 101\t| Samples: 12928, Loss: 0.031105072423815727\t| precision: 0.5458735162941202\n",
            "epoch: 15\t| Cnt: 201\t| Samples: 25728, Loss: 0.030626867301762103\t| precision: 0.5661505255279317\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.52438993117271, auroc: 0.8913064147395816,\n",
            "Epoch: 15 completed. \t time: 237.81542229652405\n",
            "Epoch: 16 training started...\n",
            "epoch: 16\t| Cnt: 1\t| Samples: 128, Loss: 0.00032469213008880617\t| precision: 0.5661816656866618\n",
            "epoch: 16\t| Cnt: 101\t| Samples: 12928, Loss: 0.03034618614241481\t| precision: 0.5590088140134628\n",
            "epoch: 16\t| Cnt: 201\t| Samples: 25728, Loss: 0.030166219417005778\t| precision: 0.5760946549440471\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.5314829777507944, auroc: 0.897816525461838,\n",
            "Epoch: 16 completed. \t time: 237.43109250068665\n",
            "Epoch: 17 training started...\n",
            "epoch: 17\t| Cnt: 1\t| Samples: 128, Loss: 0.0003051028586924076\t| precision: 0.5648804910961994\n",
            "epoch: 17\t| Cnt: 101\t| Samples: 12928, Loss: 0.030034416131675245\t| precision: 0.566251751661718\n",
            "epoch: 17\t| Cnt: 201\t| Samples: 25728, Loss: 0.030035073682665826\t| precision: 0.5685632088436248\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.5377466574934318, auroc: 0.9022816138121964,\n",
            "Epoch: 17 completed. \t time: 238.63331389427185\n",
            "Epoch: 18 training started...\n",
            "epoch: 18\t| Cnt: 1\t| Samples: 128, Loss: 0.0002762094140052795\t| precision: 0.5784735090785127\n",
            "epoch: 18\t| Cnt: 101\t| Samples: 12928, Loss: 0.029712164066731928\t| precision: 0.5682272805800037\n",
            "epoch: 18\t| Cnt: 201\t| Samples: 25728, Loss: 0.029696146920323373\t| precision: 0.5979133148825795\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.5424199118575876, auroc: 0.9064548056579119,\n",
            "Epoch: 18 completed. \t time: 238.02696919441223\n",
            "Epoch: 19 training started...\n",
            "epoch: 19\t| Cnt: 1\t| Samples: 128, Loss: 0.0002936357632279396\t| precision: 0.5796344743564238\n",
            "epoch: 19\t| Cnt: 101\t| Samples: 12928, Loss: 0.0295197931304574\t| precision: 0.5757284996574286\n",
            "epoch: 19\t| Cnt: 201\t| Samples: 25728, Loss: 0.029135270565748213\t| precision: 0.5558153206046172\n",
            "** ** * Saving fine - tuned model ** ** * \n",
            "precision : 0.5464200616220897, auroc: 0.909028122184602,\n",
            "Epoch: 19 completed. \t time: 238.11680841445923\n",
            "Best pre=0.5464200616220897, auroc: 0.909028122184602\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "optim_config = {\n",
        "    'lr': optim_config['lr'],\n",
        "    'warmup_proportion': 0.1\n",
        "}\n",
        "optim = optimizer.adam(params=list(\n",
        "    model.named_parameters()), config=optim_config)\n",
        "\n",
        "best_pre = 0\n",
        "best_roc = 0\n",
        "\n",
        "for e in range(global_params['num_epochs']):\n",
        "    print(f\"Epoch: {e} training started...\")\n",
        "    \n",
        "    start = time.time()\n",
        "    train(e)\n",
        "\n",
        "    aps, roc, test_loss = evaluation()\n",
        "    if aps > best_pre:\n",
        "        # Save a trained model\n",
        "        print(\"** ** * Saving fine - tuned model ** ** * \")\n",
        "        model_to_save = model.module if hasattr(\n",
        "            model, 'module') else model  # Only save the model it-self\n",
        "        output_model_file = os.path.join(file_config['model_path'], file_config['model_name'])\n",
        "        \n",
        "        utils.create_folder(file_config['model_path'])\n",
        "        if file_config['model_name']:\n",
        "            torch.save(model_to_save.state_dict(), output_model_file)\n",
        "\n",
        "        best_pre = aps\n",
        "        best_roc = roc\n",
        "\n",
        "    print('precision : {}, auroc: {},'.format(aps, roc))\n",
        "    print(f\"Epoch: {e} completed. \\t time: {time.time()-start}\")\n",
        "\n",
        "print(f\"Best pre={best_pre}, auroc: {best_roc}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}