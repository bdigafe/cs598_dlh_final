{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VDIyndGVBJZh"
      },
      "source": [
        "### Task 3: Next 6-months prediction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qtTLL2DrBJZk"
      },
      "source": [
        "#### Fine tunning\n",
        "\n",
        "BERT generates word embeddings for the vocabulary words included in the corpus. In other words, each word or medical diganoses in our case, is mapped to a vector. In contrast to Word2Vec, BERT generates code that captures the local context of words and therefore provides a better representation of the word.\n",
        "\n",
        "MLM (Masked Language Model) is used for optimization, which we already performed on the 2nd task. In this step, we will add a task to predict the diagnosis codes after 6-months from a randomly selected visit date. This new task will be trained and the word embeddings will be fine tuned as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dnOQmjBmQPrk"
      },
      "outputs": [],
      "source": [
        "local_mode = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCUA0c8cBgHD",
        "outputId": "5cf5c646-e1be-4f1c-e9fd-364156ade707"
      },
      "outputs": [],
      "source": [
        "if not local_mode:\n",
        "  !mkdir commons\n",
        "  !wget -P commons https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/commons/utils.py\n",
        "  !wget -P commons https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/commons/__init__.py\n",
        "\n",
        "  !mkdir models\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/MLM.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/NextXVisit.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/optimizer.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/BertConfig.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/__init__.py\n",
        "\n",
        "  !mkdir data\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/ages.pkl\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/concept.pkl\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/condition_codes.pkl\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/conditions.pkl\n",
        "\n",
        "  !mkdir saved_models\n",
        "  !wget -P saved_models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/saved_models/mlm128.pt\n",
        "\n",
        "  !mkdir images\n",
        "  !wget -P images https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/cdm54.png\n",
        "  !wget -P images https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/behrt_embeddings.png\n",
        "  !wget -P images https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/behrt_model.png\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWpwpgYeZGJg",
        "outputId": "3e8071de-e392-4b68-f9dc-2c33b10102d7"
      },
      "outputs": [],
      "source": [
        "if not local_mode:\n",
        "  %pip install pytorch_pretrained_bert "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXiIjJdbQPrl"
      },
      "source": [
        "### File and model paramters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IayjuaWgBJZm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import commons.utils as utils\n",
        "from models import optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_pretrained_bert as Bert\n",
        "\n",
        "# set random seed for reproducibility\n",
        "seed = 123\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ['PYTHONASHSEED'] = str(seed)\n",
        "\n",
        "global_params = {\n",
        "    'max_seq_len': 256,\n",
        "    'max_age' : 110,\n",
        "    'age_month' : 1,\n",
        "    'batch_size': 128,\n",
        "    'num_epochs': 20,\n",
        "    'min_visit': 5,\n",
        "    'next_x_months': 6,\n",
        "    \n",
        "    'gradient_accumulation_steps': 1,\n",
        "    'training_sample' : 0,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "}\n",
        "\n",
        "optim_config = {\n",
        "    'lr': 3e-5,\n",
        "    'warmup_proportion': 0.1,\n",
        "    'weight_decay': 0.01\n",
        "}\n",
        "\n",
        "file_config = {\n",
        "    'vocab': ('C:/Birhanu/Education/UIL/cs598/Final/data/' if local_mode else 'data/') + 'condition_codes.pkl',\n",
        "    'data': ('C:/Birhanu/Education/UIL/cs598/Final/data/' if local_mode else 'data/' ) + 'conditions.pkl',\n",
        "    'ages' : ('C:/Birhanu/Education/UIL/cs598/Final/data/' if local_mode else 'data/' ) + 'ages.pkl',\n",
        "\n",
        "    'model_path': 'C:/Birhanu/Education/UIL/cs598/Final/saved_models/' if local_mode else 'saved_models/', \n",
        "    'model_name': 'nextxm-model',  # model name\n",
        "\n",
        "    # MLM pretrained model path\n",
        "    'pretrainModel': 'C:/Birhanu/Education/UIL/cs598/Final/saved_models/mlm128.pt' if local_mode else 'saved_models/mlm128.pt',\n",
        "\n",
        "    # log file\n",
        "    'log_file_name': 'nextxm-log',  # log path\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkM2z7YqH0K6",
        "outputId": "6465a582-24ae-4105-c297-010adc96ebd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "print(global_params[\"device\"])\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt_kVGeZBJZo"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf_4wsLcBJZo",
        "outputId": "00f75a37-6e81-471e-c02f-f1df2423f263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conditions vocab file: C:/Birhanu/Education/UIL/cs598/Final/data/condition_codes.pkl\n"
          ]
        }
      ],
      "source": [
        "# Load conditions codes (word vocabs)\n",
        "print(f\"Conditions vocab file: {file_config['vocab']}\")\n",
        "vocab_conditions = utils.get_codes_vocab(file_config[\"vocab\"])\n",
        "code2idx = vocab_conditions[\"token2idx\"]\n",
        "\n",
        "# Create age vocab\n",
        "vocab_age = utils.age_vocab(global_params[\"max_age\"], global_params[\"age_month\"])\n",
        "age2idx = vocab_age[0]\n",
        "\n",
        "# Load data\n",
        "data_conditions = utils.load_data(file_config[\"data\"], sample_size=global_params[\"training_sample\"])\n",
        "data_age_seqs = utils.load_data(file_config[\"ages\"], sample_size=global_params[\"training_sample\"])\n",
        "\n",
        "# Split data into train, validation, and test\n",
        "train_data, test_data = utils.split_data(\n",
        "    data_conditions, data_age_seqs, train_ratio=0.8, min_size=3, num_months=global_params[\"next_x_months\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data size: 31057\n",
            "Test data size: 7673\n",
            "['35207924', '35211387', 'SEP', '1569634', 'SEP', '1576063', 'SEP', '1569447', '1572198', 'SEP', '1569457', 'SEP', '1569439', 'SEP', '1576063', 'SEP', '1568524', 'SEP', '1576063', 'SEP', '1576063', 'SEP']\n",
            "['230', '230', '230', '262', '262', '263', '263', '267', '267', '267', '268', '268', '268', '268', '268', '268', '268', '268', '268', '268', '268', '268']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train data size: {len(train_data)}\")\n",
        "print(f\"Test data size: {len(test_data)}\")\n",
        "\n",
        "print(train_data[\"visit\"].iloc[0])\n",
        "print(train_data[\"age\"].iloc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m-yUy_LjQPrn"
      },
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    # number of disease + symbols for word embedding\n",
        "    'vocab_size': len(vocab_conditions['token2idx'].keys()),\n",
        "\n",
        "    # word embedding and seg embedding hidden size\n",
        "    'hidden_size': 288, \n",
        "\n",
        "    # number of vocab for seg embedding\n",
        "    'seg_vocab_size': 2, \n",
        "\n",
        "    # number of vocab for age embedding\n",
        "    'age_vocab_size': len(vocab_age[0].keys()),\n",
        "\n",
        "    # maximum number of tokens\n",
        "    'max_position_embedding': global_params['max_seq_len'],\n",
        "\n",
        "    # dropout rate\n",
        "    'hidden_dropout_prob': 0.2, \n",
        "\n",
        "    # number of multi-head attention layers required\n",
        "    'num_hidden_layers': 6,  \n",
        "\n",
        "    # number of attention heads\n",
        "    'num_attention_heads': 12,\n",
        "\n",
        "    # multi-head attention dropout rate  \n",
        "    'attention_probs_dropout_prob': 0.22,  \n",
        "\n",
        "    # the size of the \"intermediate\" layer in the transformer encoder\n",
        "    'intermediate_size': 512,\n",
        "\n",
        "    # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
        "    'hidden_act': 'gelu',\n",
        "    'initializer_range': 0.02,  # parameter weight initializer range\n",
        "}\n",
        "\n",
        "feature_dict = {\n",
        "    'age': True,\n",
        "    'seg': True,\n",
        "    'posi': True\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "smgL5LjrQPrn"
      },
      "outputs": [],
      "source": [
        "class NextVisit(Dataset):\n",
        "    def __init__(self, token2idx, diag2idx, age2idx, dataframe, max_len, max_age=110, min_visit=5):\n",
        "        # dataframe preproecssing\n",
        "        # filter out the patient with number of visits less than min_visit\n",
        "        self.vocab = token2idx\n",
        "        self.label_vocab = diag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.code = dataframe.visit\n",
        "        self.age = dataframe.age\n",
        "        self.label = dataframe.label\n",
        "        self.patid = dataframe.pid\n",
        "\n",
        "        self.age2idx = age2idx\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        return: age, code, position, segmentation, mask, label\n",
        "        \"\"\"\n",
        "        # cut data\n",
        "        age = self.age[index]\n",
        "        code = self.code[index]\n",
        "        label = self.label[index]\n",
        "        patid = self.patid[index]\n",
        "\n",
        "        # extract data\n",
        "        age = age[(-self.max_len+1):]\n",
        "        code = code[(-self.max_len+1):]\n",
        "\n",
        "        # avoid data cut with first element to be 'SEP'\n",
        "        if code[0] != 'SEP':\n",
        "            code = np.append(np.array(['CLS']), code)\n",
        "            age = np.append(np.array(age[0]), age)\n",
        "        else:\n",
        "            code[0] = 'CLS'\n",
        "\n",
        "        # mask 0:len(code) to 1, padding to be 0\n",
        "        mask = np.ones(self.max_len)\n",
        "        mask[len(code):] = 0\n",
        "\n",
        "        # pad age sequence and code sequence\n",
        "        age = utils.seq_padding(age, self.max_len, token2idx=self.age2idx)\n",
        "\n",
        "        tokens, code = utils.code2index(code, self.vocab)\n",
        "        _, label = utils.code2index(label, self.label_vocab)\n",
        "\n",
        "        # get position code and segment code\n",
        "        tokens = utils.seq_padding(tokens, self.max_len)\n",
        "        position = utils.position_idx(tokens)\n",
        "        segment = utils.index_seg(tokens)\n",
        "\n",
        "        # pad code and label\n",
        "        code = utils.seq_padding(code, self.max_len, symbol=self.vocab['PAD'])\n",
        "        label = utils.seq_padding(label, self.max_len, symbol=-1)\n",
        "\n",
        "        return torch.LongTensor(age), \\\n",
        "              torch.LongTensor(code), \\\n",
        "              torch.LongTensor(position), \\\n",
        "              torch.LongTensor(segment), \\\n",
        "              torch.LongTensor(mask), \\\n",
        "              torch.LongTensor(label), \\\n",
        "              torch.LongTensor([int(patid)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKVoJMjZVSZ9",
        "outputId": "d50772d8-d6f6-43fd-ed83-f770c126375b"
      },
      "outputs": [],
      "source": [
        "# Test batch data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def inspect_batch():\n",
        "    dataset = NextVisit(token2idx=code2idx, diag2idx=code2idx,\n",
        "                 age2idx=age2idx, dataframe=train_data, \n",
        "                 max_len=global_params['max_seq_len'])\n",
        "    \n",
        "    train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)\n",
        "    loader_iter = iter(train_loader)\n",
        "    batch = next(loader_iter)\n",
        "\n",
        "    batch = tuple(t.to(global_params['device']) for t in batch)\n",
        "    age_ids, input_ids, posi_ids, segment_ids, attMask, label, masked_label = batch\n",
        "    \n",
        "    # Token codes\n",
        "    p = 0\n",
        "    print(f\"Input:\\n {input_ids}\\n\")\n",
        "    print(f\"Age:\\n{age_ids}\\n\")\n",
        "    print(f\"Positions:\\n{posi_ids}\\n\")\n",
        "    print(f\"Segments:\\n{segment_ids}\\n\")\n",
        "\n",
        "    print(f\"attMask: {attMask}\")\n",
        "    print(f\"Labels: {label}\")\n",
        "\n",
        "inspect_batch()\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a-b9og3rQPro"
      },
      "outputs": [],
      "source": [
        "class BertConfig(Bert.modeling.BertConfig):\n",
        "    def __init__(self, config):\n",
        "        super(BertConfig, self).__init__(\n",
        "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
        "            hidden_size=config['hidden_size'],\n",
        "            num_hidden_layers=config.get('num_hidden_layers'),\n",
        "            num_attention_heads=config.get('num_attention_heads'),\n",
        "            intermediate_size=config.get('intermediate_size'),\n",
        "            hidden_act=config.get('hidden_act'),\n",
        "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
        "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
        "            max_position_embeddings=config.get('max_position_embedding'),\n",
        "            initializer_range=config.get('initializer_range'),\n",
        "        )\n",
        "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
        "        self.age_vocab_size = config.get('age_vocab_size')\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, segment, age\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, feature_dict):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.feature_dict = feature_dict\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(\n",
        "            config.vocab_size, config.hidden_size)\n",
        "        self.segment_embeddings = nn.Embedding(\n",
        "            config.seg_vocab_size, config.hidden_size)\n",
        "        self.age_embeddings = nn.Embedding(\n",
        "            config.age_vocab_size, config.hidden_size)\n",
        "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
        "            from_pretrained(embeddings=self._init_posi_embedding(\n",
        "                config.max_position_embeddings, config.hidden_size))\n",
        "\n",
        "        self.LayerNorm = Bert.modeling.BertLayerNorm(\n",
        "            config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, word_ids, age_ids=None, seg_ids=None, posi_ids=None, age=True):\n",
        "        if seg_ids is None:\n",
        "            seg_ids = torch.zeros_like(word_ids)\n",
        "        if age_ids is None:\n",
        "            age_ids = torch.zeros_like(word_ids)\n",
        "        if posi_ids is None:\n",
        "            posi_ids = torch.zeros_like(word_ids)\n",
        "\n",
        "        word_embed = self.word_embeddings(word_ids)\n",
        "        segment_embed = self.segment_embeddings(seg_ids)\n",
        "        age_embed = self.age_embeddings(age_ids)\n",
        "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
        "\n",
        "        embeddings = word_embed\n",
        "\n",
        "        if self.feature_dict['age']:\n",
        "            embeddings = embeddings + age_embed\n",
        "        if self.feature_dict['seg']:\n",
        "            embeddings = embeddings + segment_embed\n",
        "        if self.feature_dict['posi']:\n",
        "            embeddings = embeddings + posi_embeddings\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
        "        def even_code(pos, idx):\n",
        "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
        "\n",
        "        def odd_code(pos, idx):\n",
        "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
        "\n",
        "        # initialize position embedding table\n",
        "        lookup_table = np.zeros(\n",
        "            (max_position_embedding, hidden_size), dtype=np.float32)\n",
        "\n",
        "        # reset table parameters with hard encoding\n",
        "        # set even dimension\n",
        "        for pos in range(max_position_embedding):\n",
        "            for idx in np.arange(0, hidden_size, step=2):\n",
        "                lookup_table[pos, idx] = even_code(pos, idx)\n",
        "                \n",
        "        # set odd dimension\n",
        "        for pos in range(max_position_embedding):\n",
        "            for idx in np.arange(1, hidden_size, step=2):\n",
        "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
        "\n",
        "        return torch.tensor(lookup_table)\n",
        "\n",
        "\n",
        "class BertModel(Bert.modeling.BertPreTrainedModel):\n",
        "    def __init__(self, config, feature_dict):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config, feature_dict)\n",
        "        self.encoder = Bert.modeling.BertEncoder(config=config)\n",
        "        self.pooler = Bert.modeling.BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if age_ids is None:\n",
        "            age_ids = torch.zeros_like(input_ids)\n",
        "        if seg_ids is None:\n",
        "            seg_ids = torch.zeros_like(input_ids)\n",
        "        if posi_ids is None:\n",
        "            posi_ids = torch.zeros_like(input_ids)\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids, age_ids, seg_ids, posi_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output\n",
        "\n",
        "\n",
        "class BertForMultiLabelPrediction(Bert.modeling.BertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels, feature_dict):\n",
        "\n",
        "        super(BertForMultiLabelPrediction, self).__init__(config)\n",
        "        \n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config, feature_dict)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, age_ids, seg_ids, posi_ids, attention_mask,\n",
        "                                     output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.MultiLabelSoftMarginLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels),\n",
        "                            labels.view(-1, self.num_labels))\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EJbUpHkUQPro"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EkOCTrpNQPro"
      },
      "outputs": [],
      "source": [
        "Dset = NextVisit(token2idx=code2idx, diag2idx=code2idx,\n",
        "                 age2idx=age2idx, dataframe=train_data, \n",
        "                 max_len=global_params['max_seq_len'])\n",
        "\n",
        "trainload = DataLoader(\n",
        "    dataset=Dset, batch_size=global_params['batch_size'], shuffle=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6_9fGilgQPro"
      },
      "outputs": [],
      "source": [
        "Dset = NextVisit(token2idx=code2idx, diag2idx=code2idx,\n",
        "                 age2idx=age2idx, dataframe=test_data, max_len=global_params['max_seq_len'])\n",
        "\n",
        "testload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=False, num_workers=3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rDLMK-y_QPrp"
      },
      "source": [
        "### Setup Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8FqEkVMQPrp",
        "outputId": "885d722e-9c71-474a-e287-7dcc52d11dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of conditions vocab: 301\n",
            "Size of Age vocab: 1322\n"
          ]
        }
      ],
      "source": [
        "print(f\"Size of conditions vocab: {len(vocab_conditions['token2idx'])}\")\n",
        "print(f\"Size of Age vocab: {len(vocab_age[0].keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T3UBaFLSBJZz"
      },
      "outputs": [],
      "source": [
        "conf = BertConfig(model_config)\n",
        "model = BertForMultiLabelPrediction(conf,  len(vocab_conditions[\"token2idx\"]), feature_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbXmFCpWqOYW",
        "outputId": "cff7db0c-3f90-4bab-962e-e08a7b86e8aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForMultiLabelPrediction(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(301, 288)\n",
              "      (segment_embeddings): Embedding(2, 288)\n",
              "      (age_embeddings): Embedding(1322, 288)\n",
              "      (posi_embeddings): Embedding(256, 288)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.2, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (key): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (value): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (dropout): Dropout(p=0.22, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=288, out_features=512, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=512, out_features=288, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (key): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (value): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (dropout): Dropout(p=0.22, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=288, out_features=512, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=512, out_features=288, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (key): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (value): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (dropout): Dropout(p=0.22, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=288, out_features=512, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=512, out_features=288, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (key): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (value): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (dropout): Dropout(p=0.22, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=288, out_features=512, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=512, out_features=288, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (key): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (value): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (dropout): Dropout(p=0.22, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=288, out_features=512, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=512, out_features=288, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (key): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (value): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (dropout): Dropout(p=0.22, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=288, out_features=288, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.2, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=288, out_features=512, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=512, out_features=288, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=288, out_features=288, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (classifier): Linear(in_features=288, out_features=301, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdFJ-lfOQPrp",
        "outputId": "967b0c1e-a6fb-4d85-9178-7e451d16c801"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Birhanu\\Education\\UIL\\cs598_dlh_final\\behrt\\behrt_NextXMonths.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Birhanu/Education/UIL/cs598_dlh_final/behrt/behrt_NextXMonths.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load pretrained model and update weights\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Birhanu/Education/UIL/cs598_dlh_final/behrt/behrt_NextXMonths.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pretrained_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(file_config[\u001b[39m'\u001b[39;49m\u001b[39mpretrainModel\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Birhanu/Education/UIL/cs598_dlh_final/behrt/behrt_NextXMonths.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model_dict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstate_dict()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Birhanu/Education/UIL/cs598_dlh_final/behrt/behrt_NextXMonths.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# 1. filter out unnecessary keys\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[0;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\serialization.py:1101\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[0;32m   1100\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1101\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1103\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
            "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\serialization.py:1083\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1079\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39muntyped()\n\u001b[0;32m   1080\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1083\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1084\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
            "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\serialization.py:215\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 215\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    216\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[0;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
            "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ],
      "source": [
        "# load pretrained model and update weights\n",
        "pretrained_dict = torch.load(file_config['pretrainModel'])\n",
        "model_dict = model.state_dict()\n",
        "\n",
        "# 1. filter out unnecessary keys\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "\n",
        "# 2. overwrite entries in the existing state dict\n",
        "model_dict.update(pretrained_dict)\n",
        "\n",
        "# 3. load the new state dict\n",
        "model.load_state_dict(model_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRdWVqJuQPrp",
        "outputId": "f1b3a131-8d34-48ea-ef01-e47a133a665e"
      },
      "outputs": [],
      "source": [
        "model = model.to(global_params['device'])\n",
        "optim = optimizer.adam(params=list(\n",
        "    model.named_parameters()), config=optim_config)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w4KnzqSUQPrq"
      },
      "source": [
        "### Evaluation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnaTVcG5QPrq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics._plot.roc_curve import RocCurveDisplay\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def precision(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output = sig(logits)\n",
        "    label, output = label.cpu(), output.detach().cpu()\n",
        "    tempprc=  average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return tempprc, output, label\n",
        "\n",
        "def precision_test(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output=sig(logits)\n",
        "    tempprc= average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    roc = roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return tempprc, roc, output, label,\n",
        "\n",
        "def auroc_test(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output=sig(logits)\n",
        "    tempprc= average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    roc = roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return roc\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BJdYiqy6QPrq"
      },
      "source": [
        "### Multi-hot Label Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "LjqNifovQPrq",
        "outputId": "db30032b-97e8-45d5-fb74-a36088428f1b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "indexes = list(vocab_conditions[\"token2idx\"].values())\n",
        "\n",
        "mlb = MultiLabelBinarizer(classes=list(indexes))\n",
        "mlb.fit([[each] for each in list(indexes)])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zZG7RjlTQPrq"
      },
      "source": [
        "### Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjKqYlcAQPrq"
      },
      "outputs": [],
      "source": [
        "def train(e):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    temp_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    cnt = 0\n",
        "    for step, batch in enumerate(trainload):\n",
        "        cnt += 1\n",
        "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
        "        targets = torch.tensor(mlb.transform(\n",
        "            targets.numpy()), dtype=torch.float32)\n",
        "\n",
        "        age_ids = age_ids.to(global_params['device'])\n",
        "        input_ids = input_ids.to(global_params['device'])\n",
        "        posi_ids = posi_ids.to(global_params['device'])\n",
        "        segment_ids = segment_ids.to(global_params['device'])\n",
        "        attMask = attMask.to(global_params['device'])\n",
        "        targets = targets.to(global_params['device'])\n",
        "\n",
        "        loss, logits = model(input_ids, age_ids, segment_ids,\n",
        "                             posi_ids, attention_mask=attMask, labels=targets)\n",
        "\n",
        "        if global_params['gradient_accumulation_steps'] > 1:\n",
        "            loss = loss/global_params['gradient_accumulation_steps']\n",
        "        loss.backward()\n",
        "\n",
        "        temp_loss += loss.item()\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            prec, a, b = precision(logits, targets)\n",
        "            print(\"epoch: {}\\t| Cnt: {}\\t| Samples: {}, Loss: {}\\t| precision: {}\".format(\n",
        "                e, cnt, nb_tr_examples, temp_loss/100, prec))\n",
        "            temp_loss = 0\n",
        "\n",
        "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "\n",
        "def evaluation():\n",
        "    model.eval()\n",
        "\n",
        "    y = []\n",
        "    y_label = []\n",
        "    tr_loss = 0\n",
        "    for step, batch in enumerate(testload):\n",
        "        model.eval()\n",
        "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
        "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
        "\n",
        "        age_ids = age_ids.to(global_params['device'])\n",
        "        input_ids = input_ids.to(global_params['device'])\n",
        "        posi_ids = posi_ids.to(global_params['device'])\n",
        "        segment_ids = segment_ids.to(global_params['device'])\n",
        "        attMask = attMask.to(global_params['device'])\n",
        "        targets = targets.to(global_params['device'])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss, logits = model(input_ids, age_ids, segment_ids,\n",
        "                                 posi_ids, attention_mask=attMask, labels=targets)\n",
        "        logits = logits.cpu()\n",
        "        targets = targets.cpu()\n",
        "\n",
        "        y_label.append(targets)\n",
        "        y.append(logits)\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "    y_label = torch.cat(y_label, dim=0)\n",
        "    y = torch.cat(y, dim=0)\n",
        "\n",
        "    aps, roc, output, label = precision_test(y, y_label)\n",
        "    return aps, roc, tr_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpft0UBsQPrr",
        "outputId": "635bebcc-3169-4b36-de58-bf0b641593b8"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "optim_config = {\n",
        "    'lr': optim_config['lr'],\n",
        "    'warmup_proportion': 0.1\n",
        "}\n",
        "optim = optimizer.adam(params=list(\n",
        "    model.named_parameters()), config=optim_config)\n",
        "\n",
        "best_pre = 0.512\n",
        "for e in range(global_params['num_epochs']):\n",
        "    print(f\"Epoch: {e} training started...\")\n",
        "    train(e)\n",
        "\n",
        "    aps, roc, test_loss = evaluation()\n",
        "    if aps > best_pre:\n",
        "        # Save a trained model\n",
        "        print(\"** ** * Saving fine - tuned model ** ** * \")\n",
        "        model_to_save = model.module if hasattr(\n",
        "            model, 'module') else model  # Only save the model it-self\n",
        "        output_model_file = os.path.join(global_params['output_dir'], global_params['best_name'])\n",
        "        \n",
        "        utils.create_folder(global_params['output_dir'])\n",
        "        if global_params['save_model']:\n",
        "            torch.save(model_to_save.state_dict(), output_model_file)\n",
        "        best_pre = aps\n",
        "\n",
        "    print('precision : {}, auroc: {},'.format(aps, roc))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
