{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VDIyndGVBJZh"
      },
      "source": [
        "### Task 3: Next 6-months prediction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qtTLL2DrBJZk"
      },
      "source": [
        "#### Fine tunning\n",
        "\n",
        "BERT generates word embeddings for the vocabulary words included in the corpus. In other words, each word or medical diganoses in our case, is mapped to a vector. In contrast to Word2Vec, BERT generates code that captures the local context of words and therefore provides a better representation of the word.\n",
        "\n",
        "MLM (Masked Language Model) is used for optimization, which we already performed on the 2nd task. In this step, we will add a task to predict the diagnosis codes after 6-months from a randomly selected visit date. This new task will be trained and the word embeddings will be fine tuned as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnOQmjBmQPrk"
      },
      "outputs": [],
      "source": [
        "local_mode = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCUA0c8cBgHD",
        "outputId": "5cf5c646-e1be-4f1c-e9fd-364156ade707"
      },
      "outputs": [],
      "source": [
        "if not local_mode:\n",
        "  !mkdir commons\n",
        "  !wget -P commons https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/commons/utils.py\n",
        "  !wget -P commons https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/commons/__init__.py\n",
        "\n",
        "  !mkdir models\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/MLM.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/NextXVisit.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/optimizer.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/BertConfig.py\n",
        "  !wget -P models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/behrt/models/__init__.py\n",
        "\n",
        "  !mkdir data\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/ages.pkl\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/concept.pkl\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/condition_codes.pkl\n",
        "  !wget -P data https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/data/conditions.pkl\n",
        "\n",
        "  !mkdir saved_models\n",
        "  !wget -P saved_models https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/saved_models/mlm128.pt\n",
        "\n",
        "  !mkdir images\n",
        "  !wget -P images https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/cdm54.png\n",
        "  !wget -P images https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/behrt_embeddings.png\n",
        "  !wget -P images https://raw.githubusercontent.com/bdigafe/cs598_dlh_final/master/images/behrt_model.png\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWpwpgYeZGJg",
        "outputId": "3e8071de-e392-4b68-f9dc-2c33b10102d7"
      },
      "outputs": [],
      "source": [
        "if not local_mode:\n",
        "  %pip install pytorch_pretrained_bert "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXiIjJdbQPrl"
      },
      "source": [
        "### File and model paramters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IayjuaWgBJZm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import commons.utils as utils\n",
        "from models import optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_pretrained_bert as Bert\n",
        "\n",
        "# set random seed for reproducibility\n",
        "seed = 123\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "os.environ['PYTHONASHSEED'] = str(seed)\n",
        "\n",
        "global_params = {\n",
        "    'max_seq_len': 256,\n",
        "    'max_age' : 110,\n",
        "    'age_month' : 1,\n",
        "    'batch_size': 128,\n",
        "    'num_epochs': 20,\n",
        "    'min_visit': 5,\n",
        "    'gradient_accumulation_steps': 1,\n",
        "    'training_sample' : 0,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "}\n",
        "\n",
        "optim_config = {\n",
        "    'lr': 3e-5,\n",
        "    'warmup_proportion': 0.1,\n",
        "    'weight_decay': 0.01\n",
        "}\n",
        "\n",
        "file_config = {\n",
        "    'vocab': ('C:/Birhanu/Education/UIL/cs598/Final/data/' if local_mode else 'data/') + 'condition_codes.pkl',\n",
        "    'data': ('C:/Birhanu/Education/UIL/cs598/Final/data/' if local_mode else 'data/' ) + 'conditions.pkl',\n",
        "    'ages' : ('C:/Birhanu/Education/UIL/cs598/Final/data/' if local_mode else 'data/' ) + 'ages.pkl',\n",
        "\n",
        "    'model_path': 'C:/Birhanu/Education/UIL/cs598/Final/saved_models/' if local_mode else 'saved_models/', \n",
        "    'model_name': 'nextxm-model',  # model name\n",
        "\n",
        "    # MLM pretrained model path\n",
        "    'pretrainModel': 'C:/Birhanu/Education/UIL/cs598/Final/saved_models/mlm128.pt' if local_mode else 'saved_models/mlm128.pt',\n",
        "\n",
        "    # log file\n",
        "    'log_file_name': 'nextxm-log',  # log path\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkM2z7YqH0K6",
        "outputId": "6465a582-24ae-4105-c297-010adc96ebd8"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "print(global_params[\"device\"])\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt_kVGeZBJZo"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf_4wsLcBJZo",
        "outputId": "00f75a37-6e81-471e-c02f-f1df2423f263"
      },
      "outputs": [],
      "source": [
        "# Load conditions codes (word vocabs)\n",
        "print(f\"Conditions vocab file: {file_config['vocab']}\")\n",
        "vocab_conditions = utils.get_codes_vocab(file_config[\"vocab\"])\n",
        "code2idx = vocab_conditions[\"token2idx\"]\n",
        "\n",
        "# Create age vocab\n",
        "vocab_age = utils.age_vocab(global_params[\"max_age\"], global_params[\"age_month\"])\n",
        "age2idx = vocab_age[0]\n",
        "\n",
        "# Load data\n",
        "data_conditions = utils.load_data(file_config[\"data\"], sample_size=global_params[\"training_sample\"])\n",
        "data_age_seqs = utils.load_data(file_config[\"ages\"], sample_size=global_params[\"training_sample\"])\n",
        "\n",
        "# Split data into train, validation, and test\n",
        "train_data, test_data = utils.split_data(data_conditions, data_age_seqs, train_ratio=0.8, min_size=4, num_months=6)\n",
        "print(train_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-yUy_LjQPrn"
      },
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    # number of disease + symbols for word embedding\n",
        "    'vocab_size': len(vocab_conditions['token2idx'].keys()),\n",
        "\n",
        "    # word embedding and seg embedding hidden size\n",
        "    'hidden_size': 288, \n",
        "\n",
        "    # number of vocab for seg embedding\n",
        "    'seg_vocab_size': 2, \n",
        "\n",
        "    # number of vocab for age embedding\n",
        "    'age_vocab_size': len(vocab_age[0].keys()),\n",
        "\n",
        "    # maximum number of tokens\n",
        "    'max_position_embedding': global_params['max_seq_len'],\n",
        "\n",
        "    # dropout rate\n",
        "    'hidden_dropout_prob': 0.2, \n",
        "\n",
        "    # number of multi-head attention layers required\n",
        "    'num_hidden_layers': 6,  \n",
        "\n",
        "    # number of attention heads\n",
        "    'num_attention_heads': 12,\n",
        "\n",
        "    # multi-head attention dropout rate  \n",
        "    'attention_probs_dropout_prob': 0.22,  \n",
        "\n",
        "    # the size of the \"intermediate\" layer in the transformer encoder\n",
        "    'intermediate_size': 512,\n",
        "\n",
        "    # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
        "    'hidden_act': 'gelu',\n",
        "    'initializer_range': 0.02,  # parameter weight initializer range\n",
        "}\n",
        "\n",
        "feature_dict = {\n",
        "    'age': True,\n",
        "    'seg': True,\n",
        "    'posi': True\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smgL5LjrQPrn"
      },
      "outputs": [],
      "source": [
        "class NextVisit(Dataset):\n",
        "    def __init__(self, token2idx, diag2idx, age2idx, dataframe, max_len, max_age=110, min_visit=5):\n",
        "        # dataframe preproecssing\n",
        "        # filter out the patient with number of visits less than min_visit\n",
        "        self.vocab = token2idx\n",
        "        self.label_vocab = diag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.code = dataframe.visit\n",
        "        self.age = dataframe.age\n",
        "        self.label = dataframe.label\n",
        "        self.patid = dataframe.pid\n",
        "\n",
        "        self.age2idx = age2idx\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        return: age, code, position, segmentation, mask, label\n",
        "        \"\"\"\n",
        "        # cut data\n",
        "        age = self.age[index]\n",
        "        code = self.code[index]\n",
        "        label = self.label[index]\n",
        "        patid = self.patid[index]\n",
        "\n",
        "        # extract data\n",
        "        age = age[(-self.max_len+1):]\n",
        "        code = code[(-self.max_len+1):]\n",
        "\n",
        "        # avoid data cut with first element to be 'SEP'\n",
        "        if code[0] != 'SEP':\n",
        "            code = np.append(np.array(['CLS']), code)\n",
        "            age = np.append(np.array(age[0]), age)\n",
        "        else:\n",
        "            code[0] = 'CLS'\n",
        "\n",
        "        # mask 0:len(code) to 1, padding to be 0\n",
        "        mask = np.ones(self.max_len)\n",
        "        mask[len(code):] = 0\n",
        "\n",
        "        # pad age sequence and code sequence\n",
        "        age = utils.seq_padding(age, self.max_len, token2idx=self.age2idx)\n",
        "\n",
        "        tokens, code = utils.code2index(code, self.vocab)\n",
        "        _, label = utils.code2index(label, self.label_vocab)\n",
        "\n",
        "        # get position code and segment code\n",
        "        tokens = utils.seq_padding(tokens, self.max_len)\n",
        "        position = utils.position_idx(tokens)\n",
        "        segment = utils.index_seg(tokens)\n",
        "\n",
        "        # pad code and label\n",
        "        code = utils.seq_padding(code, self.max_len, symbol=self.vocab['PAD'])\n",
        "        label = utils.seq_padding(label, self.max_len, symbol=-1)\n",
        "\n",
        "        return torch.LongTensor(age), \\\n",
        "              torch.LongTensor(code), \\\n",
        "              torch.LongTensor(position), \\\n",
        "              torch.LongTensor(segment), \\\n",
        "              torch.LongTensor(mask), \\\n",
        "              torch.LongTensor(label), \\\n",
        "              torch.LongTensor([int(patid)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKVoJMjZVSZ9",
        "outputId": "d50772d8-d6f6-43fd-ed83-f770c126375b"
      },
      "outputs": [],
      "source": [
        "# Test batch data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def inspect_batch():\n",
        "    dataset = NextVisit(token2idx=code2idx, diag2idx=code2idx,\n",
        "                 age2idx=age2idx, dataframe=train_data, \n",
        "                 max_len=global_params['max_seq_len'])\n",
        "    \n",
        "    train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)\n",
        "    loader_iter = iter(train_loader)\n",
        "    batch = next(loader_iter)\n",
        "\n",
        "    batch = tuple(t.to(global_params['device']) for t in batch)\n",
        "    age_ids, input_ids, posi_ids, segment_ids, attMask, label, masked_label = batch\n",
        "    \n",
        "    # Token codes\n",
        "    p = 15\n",
        "    print(f\"Input:\\n {input_ids}\\n\")\n",
        "    print(f\"Age:\\n{age_ids}\\n\")\n",
        "    print(f\"Positions:\\n{posi_ids}\\n\")\n",
        "    print(f\"Segments:\\n{segment_ids}\\n\")\n",
        "\n",
        "    print(f\"attMask: {attMask}\")\n",
        "    print(f\"Labels: {label}\")\n",
        "\n",
        "inspect_batch()\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-b9og3rQPro"
      },
      "outputs": [],
      "source": [
        "class BertConfig(Bert.modeling.BertConfig):\n",
        "    def __init__(self, config):\n",
        "        super(BertConfig, self).__init__(\n",
        "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
        "            hidden_size=config['hidden_size'],\n",
        "            num_hidden_layers=config.get('num_hidden_layers'),\n",
        "            num_attention_heads=config.get('num_attention_heads'),\n",
        "            intermediate_size=config.get('intermediate_size'),\n",
        "            hidden_act=config.get('hidden_act'),\n",
        "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
        "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
        "            max_position_embeddings=config.get('max_position_embedding'),\n",
        "            initializer_range=config.get('initializer_range'),\n",
        "        )\n",
        "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
        "        self.age_vocab_size = config.get('age_vocab_size')\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, segment, age\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, feature_dict):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.feature_dict = feature_dict\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(\n",
        "            config.vocab_size, config.hidden_size)\n",
        "        self.segment_embeddings = nn.Embedding(\n",
        "            config.seg_vocab_size, config.hidden_size)\n",
        "        self.age_embeddings = nn.Embedding(\n",
        "            config.age_vocab_size, config.hidden_size)\n",
        "        self.posi_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\\\n",
        "            from_pretrained(embeddings=self._init_posi_embedding(\n",
        "                config.max_position_embeddings, config.hidden_size))\n",
        "\n",
        "        self.LayerNorm = Bert.modeling.BertLayerNorm(\n",
        "            config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, word_ids, age_ids=None, seg_ids=None, posi_ids=None, age=True):\n",
        "        if seg_ids is None:\n",
        "            seg_ids = torch.zeros_like(word_ids)\n",
        "        if age_ids is None:\n",
        "            age_ids = torch.zeros_like(word_ids)\n",
        "        if posi_ids is None:\n",
        "            posi_ids = torch.zeros_like(word_ids)\n",
        "\n",
        "        word_embed = self.word_embeddings(word_ids)\n",
        "        segment_embed = self.segment_embeddings(seg_ids)\n",
        "        age_embed = self.age_embeddings(age_ids)\n",
        "        posi_embeddings = self.posi_embeddings(posi_ids)\n",
        "\n",
        "        embeddings = word_embed\n",
        "\n",
        "        if self.feature_dict['age']:\n",
        "            embeddings = embeddings + age_embed\n",
        "        if self.feature_dict['seg']:\n",
        "            embeddings = embeddings + segment_embed\n",
        "        if self.feature_dict['posi']:\n",
        "            embeddings = embeddings + posi_embeddings\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "    def _init_posi_embedding(self, max_position_embedding, hidden_size):\n",
        "        def even_code(pos, idx):\n",
        "            return np.sin(pos/(10000**(2*idx/hidden_size)))\n",
        "\n",
        "        def odd_code(pos, idx):\n",
        "            return np.cos(pos/(10000**(2*idx/hidden_size)))\n",
        "\n",
        "        # initialize position embedding table\n",
        "        lookup_table = np.zeros(\n",
        "            (max_position_embedding, hidden_size), dtype=np.float32)\n",
        "\n",
        "        # reset table parameters with hard encoding\n",
        "        # set even dimension\n",
        "        for pos in range(max_position_embedding):\n",
        "            for idx in np.arange(0, hidden_size, step=2):\n",
        "                lookup_table[pos, idx] = even_code(pos, idx)\n",
        "                \n",
        "        # set odd dimension\n",
        "        for pos in range(max_position_embedding):\n",
        "            for idx in np.arange(1, hidden_size, step=2):\n",
        "                lookup_table[pos, idx] = odd_code(pos, idx)\n",
        "\n",
        "        return torch.tensor(lookup_table)\n",
        "\n",
        "\n",
        "class BertModel(Bert.modeling.BertPreTrainedModel):\n",
        "    def __init__(self, config, feature_dict):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config, feature_dict)\n",
        "        self.encoder = Bert.modeling.BertEncoder(config=config)\n",
        "        self.pooler = Bert.modeling.BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if age_ids is None:\n",
        "            age_ids = torch.zeros_like(input_ids)\n",
        "        if seg_ids is None:\n",
        "            seg_ids = torch.zeros_like(input_ids)\n",
        "        if posi_ids is None:\n",
        "            posi_ids = torch.zeros_like(input_ids)\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids, age_ids, seg_ids, posi_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output\n",
        "\n",
        "\n",
        "class BertForMultiLabelPrediction(Bert.modeling.BertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels, feature_dict):\n",
        "\n",
        "        super(BertForMultiLabelPrediction, self).__init__(config)\n",
        "        \n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config, feature_dict)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, age_ids=None, seg_ids=None, posi_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, age_ids, seg_ids, posi_ids, attention_mask,\n",
        "                                     output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.MultiLabelSoftMarginLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels),\n",
        "                            labels.view(-1, self.num_labels))\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EJbUpHkUQPro"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkOCTrpNQPro"
      },
      "outputs": [],
      "source": [
        "Dset = NextVisit(token2idx=code2idx, diag2idx=code2idx,\n",
        "                 age2idx=age2idx, dataframe=train_data, \n",
        "                 max_len=global_params['max_seq_len'])\n",
        "\n",
        "trainload = DataLoader(\n",
        "    dataset=Dset, batch_size=global_params['batch_size'], shuffle=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_9fGilgQPro"
      },
      "outputs": [],
      "source": [
        "Dset = NextVisit(token2idx=code2idx, diag2idx=code2idx,\n",
        "                 age2idx=age2idx, dataframe=test_data, max_len=global_params['max_seq_len'])\n",
        "\n",
        "testload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=False, num_workers=3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rDLMK-y_QPrp"
      },
      "source": [
        "### Setup Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8FqEkVMQPrp",
        "outputId": "885d722e-9c71-474a-e287-7dcc52d11dcf"
      },
      "outputs": [],
      "source": [
        "print(f\"Size of conditions vocab: {len(vocab_conditions['token2idx'])}\")\n",
        "print(f\"Size of Age vocab: {len(vocab_age[0].keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3UBaFLSBJZz"
      },
      "outputs": [],
      "source": [
        "conf = BertConfig(model_config)\n",
        "model = BertForMultiLabelPrediction(conf,  len(vocab_conditions[\"token2idx\"]), feature_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbXmFCpWqOYW",
        "outputId": "cff7db0c-3f90-4bab-962e-e08a7b86e8aa"
      },
      "outputs": [],
      "source": [
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdFJ-lfOQPrp",
        "outputId": "967b0c1e-a6fb-4d85-9178-7e451d16c801"
      },
      "outputs": [],
      "source": [
        "# load pretrained model and update weights\n",
        "pretrained_dict = torch.load(file_config['pretrainModel'])\n",
        "model_dict = model.state_dict()\n",
        "\n",
        "# 1. filter out unnecessary keys\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "\n",
        "# 2. overwrite entries in the existing state dict\n",
        "model_dict.update(pretrained_dict)\n",
        "\n",
        "# 3. load the new state dict\n",
        "model.load_state_dict(model_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRdWVqJuQPrp",
        "outputId": "f1b3a131-8d34-48ea-ef01-e47a133a665e"
      },
      "outputs": [],
      "source": [
        "model = model.to(global_params['device'])\n",
        "optim = optimizer.adam(params=list(\n",
        "    model.named_parameters()), config=optim_config)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w4KnzqSUQPrq"
      },
      "source": [
        "### Evaluation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnaTVcG5QPrq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics._plot.roc_curve import RocCurveDisplay\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def precision(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output = sig(logits)\n",
        "    label, output = label.cpu(), output.detach().cpu()\n",
        "    tempprc=  average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return tempprc, output, label\n",
        "\n",
        "def precision_test(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output=sig(logits)\n",
        "    tempprc= average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    roc = roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return tempprc, roc, output, label,\n",
        "\n",
        "def auroc_test(logits, label):\n",
        "    sig = nn.Sigmoid()\n",
        "    output=sig(logits)\n",
        "    tempprc= average_precision_score(label.numpy(),output.numpy(), average='samples')\n",
        "    roc = roc_auc_score(label.numpy(),output.numpy(), average='samples')\n",
        "    return roc\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BJdYiqy6QPrq"
      },
      "source": [
        "### Multi-hot Label Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "LjqNifovQPrq",
        "outputId": "db30032b-97e8-45d5-fb74-a36088428f1b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "indexes = list(vocab_conditions[\"token2idx\"].values())\n",
        "\n",
        "mlb = MultiLabelBinarizer(classes=list(indexes))\n",
        "mlb.fit([[each] for each in list(indexes)])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zZG7RjlTQPrq"
      },
      "source": [
        "### Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjKqYlcAQPrq"
      },
      "outputs": [],
      "source": [
        "def train(e):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    temp_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    cnt = 0\n",
        "    for step, batch in enumerate(trainload):\n",
        "        cnt += 1\n",
        "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
        "        targets = torch.tensor(mlb.transform(\n",
        "            targets.numpy()), dtype=torch.float32)\n",
        "\n",
        "        age_ids = age_ids.to(global_params['device'])\n",
        "        input_ids = input_ids.to(global_params['device'])\n",
        "        posi_ids = posi_ids.to(global_params['device'])\n",
        "        segment_ids = segment_ids.to(global_params['device'])\n",
        "        attMask = attMask.to(global_params['device'])\n",
        "        targets = targets.to(global_params['device'])\n",
        "\n",
        "        loss, logits = model(input_ids, age_ids, segment_ids,\n",
        "                             posi_ids, attention_mask=attMask, labels=targets)\n",
        "\n",
        "        if global_params['gradient_accumulation_steps'] > 1:\n",
        "            loss = loss/global_params['gradient_accumulation_steps']\n",
        "        loss.backward()\n",
        "\n",
        "        temp_loss += loss.item()\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            prec, a, b = precision(logits, targets)\n",
        "            print(\"epoch: {}\\t| Cnt: {}\\t| Samples: {}, Loss: {}\\t| precision: {}\".format(\n",
        "                e, cnt, nb_tr_examples, temp_loss/100, prec))\n",
        "            temp_loss = 0\n",
        "\n",
        "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "\n",
        "def evaluation():\n",
        "    model.eval()\n",
        "\n",
        "    y = []\n",
        "    y_label = []\n",
        "    tr_loss = 0\n",
        "    for step, batch in enumerate(testload):\n",
        "        model.eval()\n",
        "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
        "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
        "\n",
        "        age_ids = age_ids.to(global_params['device'])\n",
        "        input_ids = input_ids.to(global_params['device'])\n",
        "        posi_ids = posi_ids.to(global_params['device'])\n",
        "        segment_ids = segment_ids.to(global_params['device'])\n",
        "        attMask = attMask.to(global_params['device'])\n",
        "        targets = targets.to(global_params['device'])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss, logits = model(input_ids, age_ids, segment_ids,\n",
        "                                 posi_ids, attention_mask=attMask, labels=targets)\n",
        "        logits = logits.cpu()\n",
        "        targets = targets.cpu()\n",
        "\n",
        "        y_label.append(targets)\n",
        "        y.append(logits)\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "    y_label = torch.cat(y_label, dim=0)\n",
        "    y = torch.cat(y, dim=0)\n",
        "\n",
        "    aps, roc, output, label = precision_test(y, y_label)\n",
        "    return aps, roc, tr_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpft0UBsQPrr",
        "outputId": "635bebcc-3169-4b36-de58-bf0b641593b8"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "optim_config = {\n",
        "    'lr': optim_config['lr'],\n",
        "    'warmup_proportion': 0.1\n",
        "}\n",
        "optim = optimizer.adam(params=list(\n",
        "    model.named_parameters()), config=optim_config)\n",
        "\n",
        "best_pre = 0.512\n",
        "for e in range(global_params['num_epochs']):\n",
        "    print(f\"Epoch: {e} training started...\")\n",
        "    train(e)\n",
        "\n",
        "    aps, roc, test_loss = evaluation()\n",
        "    if aps > best_pre:\n",
        "        # Save a trained model\n",
        "        print(\"** ** * Saving fine - tuned model ** ** * \")\n",
        "        model_to_save = model.module if hasattr(\n",
        "            model, 'module') else model  # Only save the model it-self\n",
        "        output_model_file = os.path.join(global_params['output_dir'], global_params['best_name'])\n",
        "        \n",
        "        utils.create_folder(global_params['output_dir'])\n",
        "        if global_params['save_model']:\n",
        "            torch.save(model_to_save.state_dict(), output_model_file)\n",
        "        best_pre = aps\n",
        "\n",
        "    print('precision : {}, auroc: {},'.format(aps, roc))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
